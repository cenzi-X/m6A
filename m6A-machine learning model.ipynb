{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratio</th>\n",
       "      <th>1659.78516</th>\n",
       "      <th>1658.79688</th>\n",
       "      <th>1657.80957</th>\n",
       "      <th>1656.82129</th>\n",
       "      <th>1655.83301</th>\n",
       "      <th>1654.84473</th>\n",
       "      <th>1653.85644</th>\n",
       "      <th>1652.86719</th>\n",
       "      <th>1651.87793</th>\n",
       "      <th>...</th>\n",
       "      <th>549.72363</th>\n",
       "      <th>548.48535</th>\n",
       "      <th>547.24805</th>\n",
       "      <th>546.00977</th>\n",
       "      <th>544.77148</th>\n",
       "      <th>543.53223</th>\n",
       "      <th>542.29395</th>\n",
       "      <th>541.05371</th>\n",
       "      <th>539.81445</th>\n",
       "      <th>538.57422</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12633</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.018209</td>\n",
       "      <td>0.021441</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>0.026196</td>\n",
       "      <td>0.027719</td>\n",
       "      <td>0.028673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006858</td>\n",
       "      <td>-0.004017</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.032307</td>\n",
       "      <td>0.041574</td>\n",
       "      <td>0.051759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7583</th>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>0.010499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004965</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.029666</td>\n",
       "      <td>0.040107</td>\n",
       "      <td>0.051720</td>\n",
       "      <td>0.064505</td>\n",
       "      <td>0.078461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.012659</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.021791</td>\n",
       "      <td>0.025063</td>\n",
       "      <td>0.027471</td>\n",
       "      <td>0.029016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034242</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.043177</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.053780</td>\n",
       "      <td>0.059706</td>\n",
       "      <td>0.066048</td>\n",
       "      <td>0.072807</td>\n",
       "      <td>0.079983</td>\n",
       "      <td>0.087575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7540</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.020609</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>0.017612</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>0.014852</td>\n",
       "      <td>0.013244</td>\n",
       "      <td>0.011484</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005685</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>-0.001767</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.006625</td>\n",
       "      <td>0.010093</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.022510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15125</th>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009586</td>\n",
       "      <td>0.010186</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>0.013202</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>0.022266</td>\n",
       "      <td>0.026498</td>\n",
       "      <td>0.031335</td>\n",
       "      <td>0.036778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.010654</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>-0.001261</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002436</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.028213</td>\n",
       "      <td>0.035949</td>\n",
       "      <td>0.044436</td>\n",
       "      <td>0.053673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.004942</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.011205</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>0.013697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014110</td>\n",
       "      <td>-0.012908</td>\n",
       "      <td>-0.010414</td>\n",
       "      <td>-0.006628</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.012486</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.031691</td>\n",
       "      <td>0.043232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.007161</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.023732</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.033756</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001322</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.025541</td>\n",
       "      <td>0.032102</td>\n",
       "      <td>0.039258</td>\n",
       "      <td>0.047010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>0.015045</td>\n",
       "      <td>0.018237</td>\n",
       "      <td>0.020673</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.023275</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006694</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.032661</td>\n",
       "      <td>0.044761</td>\n",
       "      <td>0.058272</td>\n",
       "      <td>0.073193</td>\n",
       "      <td>0.089523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007483</td>\n",
       "      <td>-0.003233</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>0.012256</td>\n",
       "      <td>0.014201</td>\n",
       "      <td>0.015762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061526</td>\n",
       "      <td>0.070562</td>\n",
       "      <td>0.079443</td>\n",
       "      <td>0.088170</td>\n",
       "      <td>0.096742</td>\n",
       "      <td>0.105159</td>\n",
       "      <td>0.113421</td>\n",
       "      <td>0.121529</td>\n",
       "      <td>0.129482</td>\n",
       "      <td>0.137280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 1016 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ratio  1659.78516  1658.79688  1657.80957  1656.82129  1655.83301  \\\n",
       "12633    0.6    0.005098    0.010038    0.014408    0.018209    0.021441   \n",
       "7583     0.3   -0.003269   -0.000298    0.002317    0.004573    0.006473   \n",
       "6709     0.3   -0.007515    0.000073    0.006797    0.012659    0.017657   \n",
       "7540     0.3    0.020609    0.019762    0.018763    0.017612    0.016308   \n",
       "15125    0.7   -0.002297   -0.001547   -0.000779    0.000005    0.000805   \n",
       "...      ...         ...         ...         ...         ...         ...   \n",
       "10955    0.5   -0.010654   -0.007203   -0.004072   -0.001261    0.001230   \n",
       "17289    0.8   -0.004942   -0.001348    0.001885    0.004757    0.007267   \n",
       "5192     0.2    0.000183    0.007161    0.013412    0.018935    0.023732   \n",
       "12172    0.6    0.000932    0.006392    0.011097    0.015045    0.018237   \n",
       "235      0.0   -0.007483   -0.003233    0.000633    0.004115    0.007213   \n",
       "\n",
       "       1654.84473  1653.85644  1652.86719  1651.87793  ...  549.72363  \\\n",
       "12633    0.024103    0.026196    0.027719    0.028673  ...  -0.006858   \n",
       "7583     0.008015    0.009201    0.010028    0.010499  ...  -0.004965   \n",
       "6709     0.021791    0.025063    0.027471    0.029016  ...   0.034242   \n",
       "7540     0.014852    0.013244    0.011484    0.009571  ...  -0.005685   \n",
       "15125    0.001623    0.002458    0.003310    0.004178  ...   0.009586   \n",
       "...           ...         ...         ...         ...  ...        ...   \n",
       "10955    0.003401    0.005253    0.006784    0.007996  ...  -0.002436   \n",
       "17289    0.009416    0.011205    0.012631    0.013697  ...  -0.014110   \n",
       "5192     0.027800    0.031142    0.033756    0.035643  ...  -0.001322   \n",
       "12172    0.020673    0.022352    0.023275    0.023442  ...  -0.006694   \n",
       "235      0.009926    0.012256    0.014201    0.015762  ...   0.061526   \n",
       "\n",
       "       548.48535  547.24805  546.00977  544.77148  543.53223  542.29395  \\\n",
       "12633  -0.004017  -0.000258   0.004419   0.010014   0.016527   0.023958   \n",
       "7583   -0.000383   0.005372   0.012298   0.020396   0.029666   0.040107   \n",
       "6709    0.038501   0.043177   0.048270   0.053780   0.059706   0.066048   \n",
       "7540   -0.003894  -0.001767   0.000695   0.003492   0.006625   0.010093   \n",
       "15125   0.010186   0.011391   0.013202   0.015618   0.018639   0.022266   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10955   0.000795   0.004777   0.009510   0.014993   0.021228   0.028213   \n",
       "17289  -0.012908  -0.010414  -0.006628  -0.001549   0.004822   0.012486   \n",
       "5192    0.001667   0.005251   0.009430   0.014205   0.019575   0.025541   \n",
       "12172  -0.001643   0.004818   0.012689   0.021970   0.032661   0.044761   \n",
       "235     0.070562   0.079443   0.088170   0.096742   0.105159   0.113421   \n",
       "\n",
       "       541.05371  539.81445  538.57422  \n",
       "12633   0.032307   0.041574   0.051759  \n",
       "7583    0.051720   0.064505   0.078461  \n",
       "6709    0.072807   0.079983   0.087575  \n",
       "7540    0.013897   0.018036   0.022510  \n",
       "15125   0.026498   0.031335   0.036778  \n",
       "...          ...        ...        ...  \n",
       "10955   0.035949   0.044436   0.053673  \n",
       "17289   0.021442   0.031691   0.043232  \n",
       "5192    0.032102   0.039258   0.047010  \n",
       "12172   0.058272   0.073193   0.089523  \n",
       "235     0.121529   0.129482   0.137280  \n",
       "\n",
       "[22000 rows x 1016 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "train=pd.read_csv('./all.csv', engine='python')\n",
    "train=shuffle(train, random_state=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#取第一列为标签，后面所有列为特征\n",
    "def prepare_x(train):\n",
    "    ndarray_train=train.values\n",
    "    features=ndarray_train[:,1:]\n",
    "    labels=ndarray_train[:,0]\n",
    "    scaler=StandardScaler().fit(features)\n",
    "    norm_features=scaler.transform(features)\n",
    "    return norm_features, labels\n",
    "\n",
    "import numpy as np\n",
    "X, Y=prepare_x(train)\n",
    "#经此之后，输入文件变成标准的标签-特征模式\n",
    "\n",
    "nb_features = 1015 \n",
    "X_train = np.zeros((len(X), nb_features, 1))\n",
    "X_train[:, :, 0] = X[:, :nb_features]\n",
    "#经此之后，改变输入特征格式\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(test_size=0.05, random_state=1)\n",
    "for train_index, test_index in sss.split(X_train, Y):\n",
    "    X_t, X_testing = X_train[train_index], X_train[test_index]\n",
    "    Y_t, Y_testing = Y[train_index], Y[test_index]\n",
    "#控制随机数种子，保证每次训练的测试集都是一样的\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(10, True, 1)\n",
    "for train_index, test_index in kfold.split(X_t):\n",
    "    X_train, X_test = X_t[train_index], X_t[test_index]\n",
    "    Y_train, Y_test = Y_t[train_index], Y_t[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1015)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               520192    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,038,689\n",
      "Trainable params: 1,038,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential, layers, regularizers\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv1D, Dropout, MaxPool1D, BatchNormalization, ReLU\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1015, 1)))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18810 samples, validate on 2090 samples\n",
      "Epoch 1/500\n",
      "18810/18810 [==============================] - 2s 124us/sample - loss: 0.0636 - mae: 0.1904 - val_loss: 0.0337 - val_mae: 0.1451\n",
      "Epoch 2/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0328 - mae: 0.1375 - val_loss: 0.0254 - val_mae: 0.1241\n",
      "Epoch 3/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0248 - mae: 0.1180 - val_loss: 0.0197 - val_mae: 0.1006\n",
      "Epoch 4/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0221 - mae: 0.1095 - val_loss: 0.0209 - val_mae: 0.1086\n",
      "Epoch 5/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0191 - mae: 0.1013 - val_loss: 0.0185 - val_mae: 0.0978\n",
      "Epoch 6/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0172 - mae: 0.0945 - val_loss: 0.0175 - val_mae: 0.0907\n",
      "Epoch 7/500\n",
      "18810/18810 [==============================] - 1s 74us/sample - loss: 0.0166 - mae: 0.0911 - val_loss: 0.0168 - val_mae: 0.0924\n",
      "Epoch 8/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0152 - mae: 0.0867 - val_loss: 0.0163 - val_mae: 0.0869\n",
      "Epoch 9/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0138 - mae: 0.0808 - val_loss: 0.0148 - val_mae: 0.0772\n",
      "Epoch 10/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0131 - mae: 0.0776 - val_loss: 0.0150 - val_mae: 0.0774\n",
      "Epoch 11/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0122 - mae: 0.0747 - val_loss: 0.0144 - val_mae: 0.0777\n",
      "Epoch 12/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0119 - mae: 0.0729 - val_loss: 0.0154 - val_mae: 0.0806\n",
      "Epoch 13/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0111 - mae: 0.0700 - val_loss: 0.0142 - val_mae: 0.0718\n",
      "Epoch 14/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0108 - mae: 0.0689 - val_loss: 0.0138 - val_mae: 0.0715\n",
      "Epoch 15/500\n",
      "18810/18810 [==============================] - 1s 74us/sample - loss: 0.0099 - mae: 0.0654 - val_loss: 0.0131 - val_mae: 0.0682\n",
      "Epoch 16/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0097 - mae: 0.0643 - val_loss: 0.0139 - val_mae: 0.0713\n",
      "Epoch 17/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0090 - mae: 0.0619 - val_loss: 0.0135 - val_mae: 0.0705\n",
      "Epoch 18/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0086 - mae: 0.0603 - val_loss: 0.0129 - val_mae: 0.0669\n",
      "Epoch 19/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0088 - mae: 0.0605 - val_loss: 0.0137 - val_mae: 0.0704\n",
      "Epoch 20/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0081 - mae: 0.0574 - val_loss: 0.0125 - val_mae: 0.0641\n",
      "Epoch 21/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0076 - mae: 0.0555 - val_loss: 0.0133 - val_mae: 0.0680\n",
      "Epoch 22/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0084 - mae: 0.0577 - val_loss: 0.0135 - val_mae: 0.0669\n",
      "Epoch 23/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0071 - mae: 0.0538 - val_loss: 0.0122 - val_mae: 0.0618\n",
      "Epoch 24/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0070 - mae: 0.0528 - val_loss: 0.0127 - val_mae: 0.0609\n",
      "Epoch 25/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0069 - mae: 0.0526 - val_loss: 0.0125 - val_mae: 0.0625\n",
      "Epoch 26/500\n",
      "18810/18810 [==============================] - 1s 74us/sample - loss: 0.0071 - mae: 0.0524 - val_loss: 0.0136 - val_mae: 0.0633\n",
      "Epoch 27/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0068 - mae: 0.0518 - val_loss: 0.0126 - val_mae: 0.0634\n",
      "Epoch 28/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0067 - mae: 0.0514 - val_loss: 0.0118 - val_mae: 0.0602\n",
      "Epoch 29/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0065 - mae: 0.0497 - val_loss: 0.0128 - val_mae: 0.0613\n",
      "Epoch 30/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0062 - mae: 0.0488 - val_loss: 0.0128 - val_mae: 0.0618\n",
      "Epoch 31/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0063 - mae: 0.0488 - val_loss: 0.0122 - val_mae: 0.0616\n",
      "Epoch 32/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0057 - mae: 0.0466 - val_loss: 0.0115 - val_mae: 0.0556\n",
      "Epoch 33/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0052 - mae: 0.0449 - val_loss: 0.0122 - val_mae: 0.0582\n",
      "Epoch 34/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0052 - mae: 0.0452 - val_loss: 0.0123 - val_mae: 0.0559\n",
      "Epoch 35/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0054 - mae: 0.0448 - val_loss: 0.0125 - val_mae: 0.0616\n",
      "Epoch 36/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0052 - mae: 0.0441 - val_loss: 0.0114 - val_mae: 0.0555\n",
      "Epoch 37/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0050 - mae: 0.0432 - val_loss: 0.0113 - val_mae: 0.0581\n",
      "Epoch 38/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0052 - mae: 0.0438 - val_loss: 0.0133 - val_mae: 0.0580\n",
      "Epoch 39/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0062 - mae: 0.0468 - val_loss: 0.0140 - val_mae: 0.0639\n",
      "Epoch 40/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0053 - mae: 0.0443 - val_loss: 0.0118 - val_mae: 0.0560\n",
      "Epoch 41/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0050 - mae: 0.0430 - val_loss: 0.0116 - val_mae: 0.0562\n",
      "Epoch 42/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0052 - mae: 0.0433 - val_loss: 0.0129 - val_mae: 0.0578\n",
      "Epoch 43/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0052 - mae: 0.0435 - val_loss: 0.0117 - val_mae: 0.0545\n",
      "Epoch 44/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0047 - mae: 0.0410 - val_loss: 0.0130 - val_mae: 0.0576\n",
      "Epoch 45/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0046 - mae: 0.0408 - val_loss: 0.0115 - val_mae: 0.0554\n",
      "Epoch 46/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0053 - mae: 0.0432 - val_loss: 0.0136 - val_mae: 0.0663\n",
      "Epoch 47/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0047 - mae: 0.0418 - val_loss: 0.0118 - val_mae: 0.0567\n",
      "Epoch 48/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0047 - mae: 0.0405 - val_loss: 0.0123 - val_mae: 0.0561\n",
      "Epoch 49/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0043 - mae: 0.0391 - val_loss: 0.0113 - val_mae: 0.0538\n",
      "Epoch 50/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0042 - mae: 0.0388 - val_loss: 0.0124 - val_mae: 0.0538\n",
      "Epoch 51/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0043 - mae: 0.0383 - val_loss: 0.0127 - val_mae: 0.0530\n",
      "Epoch 52/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0046 - mae: 0.0396 - val_loss: 0.0121 - val_mae: 0.0540\n",
      "Epoch 53/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0043 - mae: 0.0389 - val_loss: 0.0114 - val_mae: 0.0536\n",
      "Epoch 54/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0045 - mae: 0.0391 - val_loss: 0.0130 - val_mae: 0.0602\n",
      "Epoch 55/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0051 - mae: 0.0420 - val_loss: 0.0117 - val_mae: 0.0543\n",
      "Epoch 56/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0043 - mae: 0.0384 - val_loss: 0.0122 - val_mae: 0.0538\n",
      "Epoch 57/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0043 - mae: 0.0381 - val_loss: 0.0112 - val_mae: 0.0516\n",
      "Epoch 58/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0045 - mae: 0.0389 - val_loss: 0.0116 - val_mae: 0.0548\n",
      "Epoch 59/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0043 - mae: 0.0381 - val_loss: 0.0105 - val_mae: 0.0499\n",
      "Epoch 60/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0039 - mae: 0.0358 - val_loss: 0.0108 - val_mae: 0.0507\n",
      "Epoch 61/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0036 - mae: 0.0347 - val_loss: 0.0104 - val_mae: 0.0484\n",
      "Epoch 62/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0034 - mae: 0.0342 - val_loss: 0.0114 - val_mae: 0.0569\n",
      "Epoch 63/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0035 - mae: 0.0342 - val_loss: 0.0121 - val_mae: 0.0533\n",
      "Epoch 64/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0034 - mae: 0.0332 - val_loss: 0.0107 - val_mae: 0.0493\n",
      "Epoch 65/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0034 - mae: 0.0333 - val_loss: 0.0108 - val_mae: 0.0524\n",
      "Epoch 66/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0038 - mae: 0.0342 - val_loss: 0.0108 - val_mae: 0.0497\n",
      "Epoch 67/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0037 - mae: 0.0348 - val_loss: 0.0114 - val_mae: 0.0516\n",
      "Epoch 68/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0033 - mae: 0.0327 - val_loss: 0.0110 - val_mae: 0.0490\n",
      "Epoch 69/500\n",
      "18810/18810 [==============================] - 2s 85us/sample - loss: 0.0030 - mae: 0.0307 - val_loss: 0.0102 - val_mae: 0.0469\n",
      "Epoch 70/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0030 - mae: 0.0316 - val_loss: 0.0106 - val_mae: 0.0477\n",
      "Epoch 71/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0032 - mae: 0.0314 - val_loss: 0.0115 - val_mae: 0.0523\n",
      "Epoch 72/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0033 - mae: 0.0329 - val_loss: 0.0111 - val_mae: 0.0508\n",
      "Epoch 73/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0035 - mae: 0.0331 - val_loss: 0.0119 - val_mae: 0.0565\n",
      "Epoch 74/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0030 - mae: 0.0310 - val_loss: 0.0109 - val_mae: 0.0479\n",
      "Epoch 75/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0034 - mae: 0.0321 - val_loss: 0.0123 - val_mae: 0.0495\n",
      "Epoch 76/500\n",
      "18810/18810 [==============================] - 2s 84us/sample - loss: 0.0045 - mae: 0.0365 - val_loss: 0.0143 - val_mae: 0.0607\n",
      "Epoch 77/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0046 - mae: 0.0385 - val_loss: 0.0121 - val_mae: 0.0542\n",
      "Epoch 78/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0039 - mae: 0.0355 - val_loss: 0.0116 - val_mae: 0.0502\n",
      "Epoch 79/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0030 - mae: 0.0310 - val_loss: 0.0107 - val_mae: 0.0492\n",
      "Epoch 80/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0031 - mae: 0.0310 - val_loss: 0.0108 - val_mae: 0.0486\n",
      "Epoch 81/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0033 - mae: 0.0321 - val_loss: 0.0119 - val_mae: 0.0508\n",
      "Epoch 82/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0030 - mae: 0.0303 - val_loss: 0.0112 - val_mae: 0.0526\n",
      "Epoch 83/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0031 - mae: 0.0315 - val_loss: 0.0108 - val_mae: 0.0510\n",
      "Epoch 84/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0030 - mae: 0.0298 - val_loss: 0.0107 - val_mae: 0.0476\n",
      "Epoch 85/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0036 - mae: 0.0321 - val_loss: 0.0110 - val_mae: 0.0495\n",
      "Epoch 86/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0032 - mae: 0.0307 - val_loss: 0.0100 - val_mae: 0.0460\n",
      "Epoch 87/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0028 - mae: 0.0300 - val_loss: 0.0105 - val_mae: 0.0472\n",
      "Epoch 88/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0029 - mae: 0.0299 - val_loss: 0.0110 - val_mae: 0.0493\n",
      "Epoch 89/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0045 - mae: 0.0372 - val_loss: 0.0106 - val_mae: 0.0497\n",
      "Epoch 90/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0028 - mae: 0.0299 - val_loss: 0.0097 - val_mae: 0.0455\n",
      "Epoch 91/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0027 - mae: 0.0287 - val_loss: 0.0111 - val_mae: 0.0487\n",
      "Epoch 92/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0028 - mae: 0.0285 - val_loss: 0.0103 - val_mae: 0.0454\n",
      "Epoch 93/500\n",
      "18810/18810 [==============================] - 1s 80us/sample - loss: 0.0025 - mae: 0.0274 - val_loss: 0.0123 - val_mae: 0.0486\n",
      "Epoch 94/500\n",
      "18810/18810 [==============================] - 1s 80us/sample - loss: 0.0041 - mae: 0.0341 - val_loss: 0.0117 - val_mae: 0.0522\n",
      "Epoch 95/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0032 - mae: 0.0306 - val_loss: 0.0111 - val_mae: 0.0465\n",
      "Epoch 96/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0033 - mae: 0.0307 - val_loss: 0.0107 - val_mae: 0.0467\n",
      "Epoch 97/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0028 - mae: 0.0283 - val_loss: 0.0112 - val_mae: 0.0478\n",
      "Epoch 98/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0026 - mae: 0.0272 - val_loss: 0.0119 - val_mae: 0.0495\n",
      "Epoch 99/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0032 - mae: 0.0301 - val_loss: 0.0111 - val_mae: 0.0481\n",
      "Epoch 100/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0032 - mae: 0.0302 - val_loss: 0.0115 - val_mae: 0.0475\n",
      "Epoch 101/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0030 - mae: 0.0292 - val_loss: 0.0102 - val_mae: 0.0466\n",
      "Epoch 102/500\n",
      "18810/18810 [==============================] - 1s 80us/sample - loss: 0.0026 - mae: 0.0269 - val_loss: 0.0117 - val_mae: 0.0497\n",
      "Epoch 103/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0031 - mae: 0.0293 - val_loss: 0.0120 - val_mae: 0.0513\n",
      "Epoch 104/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0029 - mae: 0.0287 - val_loss: 0.0114 - val_mae: 0.0488\n",
      "Epoch 105/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0029 - mae: 0.0285 - val_loss: 0.0112 - val_mae: 0.0470\n",
      "Epoch 106/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0026 - mae: 0.0268 - val_loss: 0.0105 - val_mae: 0.0474\n",
      "Epoch 107/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0025 - mae: 0.0264 - val_loss: 0.0110 - val_mae: 0.0460\n",
      "Epoch 108/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0022 - mae: 0.0255 - val_loss: 0.0112 - val_mae: 0.0461\n",
      "Epoch 109/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0020 - mae: 0.0234 - val_loss: 0.0105 - val_mae: 0.0430\n",
      "Epoch 110/500\n",
      "18810/18810 [==============================] - 2s 83us/sample - loss: 0.0033 - mae: 0.0290 - val_loss: 0.0117 - val_mae: 0.0479\n",
      "Epoch 111/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0027 - mae: 0.0277 - val_loss: 0.0112 - val_mae: 0.0465\n",
      "Epoch 112/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0024 - mae: 0.0256 - val_loss: 0.0118 - val_mae: 0.0486\n",
      "Epoch 113/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0027 - mae: 0.0276 - val_loss: 0.0110 - val_mae: 0.0471\n",
      "Epoch 114/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0030 - mae: 0.0284 - val_loss: 0.0107 - val_mae: 0.0461\n",
      "Epoch 115/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0029 - mae: 0.0285 - val_loss: 0.0112 - val_mae: 0.0475\n",
      "Epoch 116/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0027 - mae: 0.0272 - val_loss: 0.0104 - val_mae: 0.0478\n",
      "Epoch 117/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0022 - mae: 0.0251 - val_loss: 0.0099 - val_mae: 0.0428\n",
      "Epoch 118/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0024 - mae: 0.0256 - val_loss: 0.0112 - val_mae: 0.0511\n",
      "Epoch 119/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0026 - mae: 0.0269 - val_loss: 0.0109 - val_mae: 0.0458\n",
      "Epoch 120/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0029 - mae: 0.0266 - val_loss: 0.0114 - val_mae: 0.0472\n",
      "Epoch 121/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0035 - mae: 0.0295 - val_loss: 0.0125 - val_mae: 0.0513\n",
      "Epoch 122/500\n",
      "18810/18810 [==============================] - 2s 83us/sample - loss: 0.0030 - mae: 0.0289 - val_loss: 0.0114 - val_mae: 0.0467\n",
      "Epoch 123/500\n",
      "18810/18810 [==============================] - 2s 94us/sample - loss: 0.0026 - mae: 0.0262 - val_loss: 0.0108 - val_mae: 0.0446\n",
      "Epoch 124/500\n",
      "18810/18810 [==============================] - 2s 90us/sample - loss: 0.0024 - mae: 0.0257 - val_loss: 0.0100 - val_mae: 0.0430\n",
      "Epoch 125/500\n",
      "18810/18810 [==============================] - 2s 86us/sample - loss: 0.0024 - mae: 0.0247 - val_loss: 0.0106 - val_mae: 0.0457\n",
      "Epoch 126/500\n",
      "18810/18810 [==============================] - 2s 86us/sample - loss: 0.0026 - mae: 0.0261 - val_loss: 0.0122 - val_mae: 0.0505\n",
      "Epoch 127/500\n",
      "18810/18810 [==============================] - 2s 88us/sample - loss: 0.0026 - mae: 0.0267 - val_loss: 0.0110 - val_mae: 0.0472\n",
      "Epoch 128/500\n",
      "18810/18810 [==============================] - 2s 84us/sample - loss: 0.0030 - mae: 0.0281 - val_loss: 0.0114 - val_mae: 0.0476\n",
      "Epoch 129/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0025 - mae: 0.0268 - val_loss: 0.0103 - val_mae: 0.0475\n",
      "Epoch 130/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0023 - mae: 0.0252 - val_loss: 0.0108 - val_mae: 0.0461\n",
      "Epoch 131/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0020 - mae: 0.0238 - val_loss: 0.0110 - val_mae: 0.0463\n",
      "Epoch 132/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0022 - mae: 0.0237 - val_loss: 0.0105 - val_mae: 0.0436\n",
      "Epoch 133/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0020 - mae: 0.0219 - val_loss: 0.0104 - val_mae: 0.0432\n",
      "Epoch 134/500\n",
      "18810/18810 [==============================] - 1s 76us/sample - loss: 0.0022 - mae: 0.0237 - val_loss: 0.0113 - val_mae: 0.0452\n",
      "Epoch 135/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0019 - mae: 0.0223 - val_loss: 0.0110 - val_mae: 0.0455\n",
      "Epoch 136/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0019 - mae: 0.0218 - val_loss: 0.0104 - val_mae: 0.0434\n",
      "Epoch 137/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0019 - mae: 0.0215 - val_loss: 0.0102 - val_mae: 0.0421\n",
      "Epoch 138/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0021 - mae: 0.0228 - val_loss: 0.0123 - val_mae: 0.0517\n",
      "Epoch 139/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0023 - mae: 0.0248 - val_loss: 0.0101 - val_mae: 0.0424\n",
      "Epoch 140/500\n",
      "18810/18810 [==============================] - 2s 89us/sample - loss: 0.0018 - mae: 0.0221 - val_loss: 0.0095 - val_mae: 0.0405\n",
      "Epoch 141/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0017 - mae: 0.0202 - val_loss: 0.0098 - val_mae: 0.0423\n",
      "Epoch 142/500\n",
      "18810/18810 [==============================] - 2s 105us/sample - loss: 0.0029 - mae: 0.0258 - val_loss: 0.0111 - val_mae: 0.0453\n",
      "Epoch 143/500\n",
      "18810/18810 [==============================] - 2s 91us/sample - loss: 0.0026 - mae: 0.0249 - val_loss: 0.0105 - val_mae: 0.0445\n",
      "Epoch 144/500\n",
      "18810/18810 [==============================] - 2s 87us/sample - loss: 0.0020 - mae: 0.0236 - val_loss: 0.0106 - val_mae: 0.0430\n",
      "Epoch 145/500\n",
      "18810/18810 [==============================] - 2s 84us/sample - loss: 0.0021 - mae: 0.0221 - val_loss: 0.0110 - val_mae: 0.0444\n",
      "Epoch 146/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0022 - mae: 0.0233 - val_loss: 0.0100 - val_mae: 0.0435\n",
      "Epoch 147/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0020 - mae: 0.0214 - val_loss: 0.0110 - val_mae: 0.0444\n",
      "Epoch 148/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0022 - mae: 0.0232 - val_loss: 0.0104 - val_mae: 0.0430\n",
      "Epoch 149/500\n",
      "18810/18810 [==============================] - 2s 83us/sample - loss: 0.0022 - mae: 0.0236 - val_loss: 0.0104 - val_mae: 0.0459\n",
      "Epoch 150/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0021 - mae: 0.0232 - val_loss: 0.0101 - val_mae: 0.0410\n",
      "Epoch 151/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0021 - mae: 0.0238 - val_loss: 0.0094 - val_mae: 0.0411\n",
      "Epoch 152/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0021 - mae: 0.0239 - val_loss: 0.0101 - val_mae: 0.0455\n",
      "Epoch 153/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0021 - mae: 0.0227 - val_loss: 0.0096 - val_mae: 0.0405\n",
      "Epoch 154/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0022 - mae: 0.0241 - val_loss: 0.0106 - val_mae: 0.0439\n",
      "Epoch 155/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0021 - mae: 0.0225 - val_loss: 0.0107 - val_mae: 0.0446\n",
      "Epoch 156/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0023 - mae: 0.0230 - val_loss: 0.0105 - val_mae: 0.0424\n",
      "Epoch 157/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0023 - mae: 0.0233 - val_loss: 0.0103 - val_mae: 0.0452\n",
      "Epoch 158/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0216 - val_loss: 0.0108 - val_mae: 0.0446\n",
      "Epoch 159/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0020 - mae: 0.0223 - val_loss: 0.0097 - val_mae: 0.0430\n",
      "Epoch 160/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0209 - val_loss: 0.0097 - val_mae: 0.0390\n",
      "Epoch 161/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0022 - mae: 0.0224 - val_loss: 0.0102 - val_mae: 0.0425\n",
      "Epoch 162/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0199 - val_loss: 0.0095 - val_mae: 0.0394\n",
      "Epoch 163/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0018 - mae: 0.0201 - val_loss: 0.0098 - val_mae: 0.0402\n",
      "Epoch 164/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0204 - val_loss: 0.0099 - val_mae: 0.0413\n",
      "Epoch 165/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0218 - val_loss: 0.0109 - val_mae: 0.0429\n",
      "Epoch 166/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0018 - mae: 0.0207 - val_loss: 0.0109 - val_mae: 0.0445\n",
      "Epoch 167/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0213 - val_loss: 0.0109 - val_mae: 0.0475\n",
      "Epoch 168/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0020 - mae: 0.0220 - val_loss: 0.0103 - val_mae: 0.0413\n",
      "Epoch 169/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0018 - mae: 0.0209 - val_loss: 0.0105 - val_mae: 0.0419\n",
      "Epoch 170/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0206 - val_loss: 0.0099 - val_mae: 0.0438\n",
      "Epoch 171/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0205 - val_loss: 0.0097 - val_mae: 0.0400\n",
      "Epoch 172/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0203 - val_loss: 0.0096 - val_mae: 0.0403\n",
      "Epoch 173/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0203 - val_loss: 0.0103 - val_mae: 0.0426\n",
      "Epoch 174/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0212 - val_loss: 0.0108 - val_mae: 0.0424\n",
      "Epoch 175/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0208 - val_loss: 0.0111 - val_mae: 0.0435\n",
      "Epoch 176/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0024 - mae: 0.0237 - val_loss: 0.0105 - val_mae: 0.0446\n",
      "Epoch 177/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0022 - mae: 0.0228 - val_loss: 0.0104 - val_mae: 0.0424\n",
      "Epoch 178/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0018 - mae: 0.0199 - val_loss: 0.0102 - val_mae: 0.0416\n",
      "Epoch 179/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0018 - mae: 0.0200 - val_loss: 0.0113 - val_mae: 0.0461\n",
      "Epoch 180/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0196 - val_loss: 0.0097 - val_mae: 0.0398\n",
      "Epoch 181/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0020 - mae: 0.0204 - val_loss: 0.0110 - val_mae: 0.0450\n",
      "Epoch 182/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0021 - mae: 0.0223 - val_loss: 0.0110 - val_mae: 0.0421\n",
      "Epoch 183/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0188 - val_loss: 0.0101 - val_mae: 0.0406\n",
      "Epoch 184/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0014 - mae: 0.0177 - val_loss: 0.0100 - val_mae: 0.0397\n",
      "Epoch 185/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0015 - mae: 0.0185 - val_loss: 0.0100 - val_mae: 0.0403\n",
      "Epoch 186/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0197 - val_loss: 0.0113 - val_mae: 0.0432\n",
      "Epoch 187/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0025 - mae: 0.0240 - val_loss: 0.0106 - val_mae: 0.0432\n",
      "Epoch 188/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0023 - mae: 0.0223 - val_loss: 0.0098 - val_mae: 0.0412\n",
      "Epoch 189/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0205 - val_loss: 0.0104 - val_mae: 0.0411\n",
      "Epoch 190/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0021 - mae: 0.0210 - val_loss: 0.0111 - val_mae: 0.0441\n",
      "Epoch 191/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0020 - mae: 0.0217 - val_loss: 0.0108 - val_mae: 0.0455\n",
      "Epoch 192/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0016 - mae: 0.0199 - val_loss: 0.0104 - val_mae: 0.0426\n",
      "Epoch 193/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0197 - val_loss: 0.0104 - val_mae: 0.0412\n",
      "Epoch 194/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0016 - mae: 0.0187 - val_loss: 0.0102 - val_mae: 0.0417\n",
      "Epoch 195/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0199 - val_loss: 0.0098 - val_mae: 0.0409\n",
      "Epoch 196/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0199 - val_loss: 0.0108 - val_mae: 0.0423\n",
      "Epoch 197/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0020 - mae: 0.0207 - val_loss: 0.0101 - val_mae: 0.0398\n",
      "Epoch 198/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0021 - mae: 0.0216 - val_loss: 0.0128 - val_mae: 0.0486\n",
      "Epoch 199/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0021 - mae: 0.0213 - val_loss: 0.0106 - val_mae: 0.0448\n",
      "Epoch 200/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0016 - mae: 0.0193 - val_loss: 0.0103 - val_mae: 0.0429\n",
      "Epoch 201/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0016 - mae: 0.0192 - val_loss: 0.0104 - val_mae: 0.0415\n",
      "Epoch 202/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0016 - mae: 0.0187 - val_loss: 0.0105 - val_mae: 0.0417\n",
      "Epoch 203/500\n",
      "18810/18810 [==============================] - 1s 74us/sample - loss: 0.0018 - mae: 0.0197 - val_loss: 0.0108 - val_mae: 0.0427\n",
      "Epoch 204/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0023 - mae: 0.0227 - val_loss: 0.0102 - val_mae: 0.0432\n",
      "Epoch 205/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0018 - mae: 0.0201 - val_loss: 0.0097 - val_mae: 0.0422\n",
      "Epoch 206/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0179 - val_loss: 0.0098 - val_mae: 0.0415\n",
      "Epoch 207/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0014 - mae: 0.0177 - val_loss: 0.0108 - val_mae: 0.0423\n",
      "Epoch 208/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0014 - mae: 0.0178 - val_loss: 0.0103 - val_mae: 0.0408\n",
      "Epoch 209/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0016 - mae: 0.0182 - val_loss: 0.0103 - val_mae: 0.0391\n",
      "Epoch 210/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0014 - mae: 0.0173 - val_loss: 0.0106 - val_mae: 0.0417\n",
      "Epoch 211/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0014 - mae: 0.0178 - val_loss: 0.0099 - val_mae: 0.0402\n",
      "Epoch 212/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0170 - val_loss: 0.0102 - val_mae: 0.0401\n",
      "Epoch 213/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0171 - val_loss: 0.0099 - val_mae: 0.0401\n",
      "Epoch 214/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0012 - mae: 0.0164 - val_loss: 0.0101 - val_mae: 0.0390\n",
      "Epoch 215/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0016 - mae: 0.0175 - val_loss: 0.0097 - val_mae: 0.0393\n",
      "Epoch 216/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0016 - mae: 0.0186 - val_loss: 0.0111 - val_mae: 0.0412\n",
      "Epoch 217/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0190 - val_loss: 0.0112 - val_mae: 0.0429\n",
      "Epoch 218/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0019 - mae: 0.0196 - val_loss: 0.0105 - val_mae: 0.0422\n",
      "Epoch 219/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0020 - mae: 0.0201 - val_loss: 0.0097 - val_mae: 0.0400\n",
      "Epoch 220/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0017 - mae: 0.0192 - val_loss: 0.0093 - val_mae: 0.0406\n",
      "Epoch 221/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0183 - val_loss: 0.0095 - val_mae: 0.0386\n",
      "Epoch 222/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0016 - mae: 0.0178 - val_loss: 0.0096 - val_mae: 0.0393\n",
      "Epoch 223/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0013 - mae: 0.0169 - val_loss: 0.0086 - val_mae: 0.0368\n",
      "Epoch 224/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0169 - val_loss: 0.0095 - val_mae: 0.0373\n",
      "Epoch 225/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0177 - val_loss: 0.0096 - val_mae: 0.0384\n",
      "Epoch 226/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0018 - mae: 0.0192 - val_loss: 0.0097 - val_mae: 0.0394\n",
      "Epoch 227/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0015 - mae: 0.0177 - val_loss: 0.0095 - val_mae: 0.0378\n",
      "Epoch 228/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0018 - mae: 0.0185 - val_loss: 0.0110 - val_mae: 0.0439\n",
      "Epoch 229/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0022 - mae: 0.0212 - val_loss: 0.0102 - val_mae: 0.0440\n",
      "Epoch 230/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0023 - mae: 0.0219 - val_loss: 0.0101 - val_mae: 0.0415\n",
      "Epoch 231/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0196 - val_loss: 0.0103 - val_mae: 0.0436\n",
      "Epoch 232/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0208 - val_loss: 0.0101 - val_mae: 0.0411\n",
      "Epoch 233/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0016 - mae: 0.0181 - val_loss: 0.0092 - val_mae: 0.0364\n",
      "Epoch 234/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0173 - val_loss: 0.0086 - val_mae: 0.0367\n",
      "Epoch 235/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0170 - val_loss: 0.0096 - val_mae: 0.0381\n",
      "Epoch 236/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0168 - val_loss: 0.0106 - val_mae: 0.0405\n",
      "Epoch 237/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0174 - val_loss: 0.0104 - val_mae: 0.0396\n",
      "Epoch 238/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0012 - mae: 0.0158 - val_loss: 0.0096 - val_mae: 0.0391\n",
      "Epoch 239/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0012 - mae: 0.0154 - val_loss: 0.0104 - val_mae: 0.0391\n",
      "Epoch 240/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0010 - mae: 0.0144 - val_loss: 0.0101 - val_mae: 0.0385\n",
      "Epoch 241/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0012 - mae: 0.0150 - val_loss: 0.0092 - val_mae: 0.0365\n",
      "Epoch 242/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0166 - val_loss: 0.0101 - val_mae: 0.0390\n",
      "Epoch 243/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0170 - val_loss: 0.0103 - val_mae: 0.0403\n",
      "Epoch 244/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0016 - mae: 0.0176 - val_loss: 0.0104 - val_mae: 0.0425\n",
      "Epoch 245/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0024 - mae: 0.0219 - val_loss: 0.0108 - val_mae: 0.0416\n",
      "Epoch 246/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0019 - mae: 0.0192 - val_loss: 0.0117 - val_mae: 0.0423\n",
      "Epoch 247/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0199 - val_loss: 0.0104 - val_mae: 0.0400\n",
      "Epoch 248/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0018 - mae: 0.0185 - val_loss: 0.0092 - val_mae: 0.0383\n",
      "Epoch 249/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0014 - mae: 0.0168 - val_loss: 0.0094 - val_mae: 0.0389\n",
      "Epoch 250/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0015 - mae: 0.0176 - val_loss: 0.0096 - val_mae: 0.0379\n",
      "Epoch 251/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0172 - val_loss: 0.0094 - val_mae: 0.0384\n",
      "Epoch 252/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0013 - mae: 0.0165 - val_loss: 0.0094 - val_mae: 0.0394\n",
      "Epoch 253/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0014 - mae: 0.0164 - val_loss: 0.0101 - val_mae: 0.0401\n",
      "Epoch 254/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0018 - mae: 0.0193 - val_loss: 0.0102 - val_mae: 0.0437\n",
      "Epoch 255/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0179 - val_loss: 0.0096 - val_mae: 0.0399\n",
      "Epoch 256/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0013 - mae: 0.0170 - val_loss: 0.0087 - val_mae: 0.0354\n",
      "Epoch 257/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0011 - mae: 0.0153 - val_loss: 0.0091 - val_mae: 0.0382\n",
      "Epoch 258/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0012 - mae: 0.0156 - val_loss: 0.0090 - val_mae: 0.0360\n",
      "Epoch 259/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0013 - mae: 0.0155 - val_loss: 0.0093 - val_mae: 0.0378\n",
      "Epoch 260/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0165 - val_loss: 0.0096 - val_mae: 0.0384\n",
      "Epoch 261/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0015 - mae: 0.0172 - val_loss: 0.0099 - val_mae: 0.0390\n",
      "Epoch 262/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0157 - val_loss: 0.0088 - val_mae: 0.0368\n",
      "Epoch 263/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0170 - val_loss: 0.0089 - val_mae: 0.0372\n",
      "Epoch 264/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0177 - val_loss: 0.0114 - val_mae: 0.0436\n",
      "Epoch 265/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0190 - val_loss: 0.0096 - val_mae: 0.0376\n",
      "Epoch 266/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0177 - val_loss: 0.0101 - val_mae: 0.0409\n",
      "Epoch 267/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0015 - mae: 0.0174 - val_loss: 0.0111 - val_mae: 0.0430\n",
      "Epoch 268/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0170 - val_loss: 0.0103 - val_mae: 0.0400\n",
      "Epoch 269/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0013 - mae: 0.0164 - val_loss: 0.0095 - val_mae: 0.0379\n",
      "Epoch 270/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0013 - mae: 0.0165 - val_loss: 0.0089 - val_mae: 0.0370\n",
      "Epoch 271/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0012 - mae: 0.0147 - val_loss: 0.0091 - val_mae: 0.0361\n",
      "Epoch 272/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 9.9052e-04 - mae: 0.0139 - val_loss: 0.0089 - val_mae: 0.0357\n",
      "Epoch 273/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0154 - val_loss: 0.0103 - val_mae: 0.0403\n",
      "Epoch 274/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0165 - val_loss: 0.0105 - val_mae: 0.0408\n",
      "Epoch 275/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0162 - val_loss: 0.0111 - val_mae: 0.0403\n",
      "Epoch 276/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0011 - mae: 0.0151 - val_loss: 0.0105 - val_mae: 0.0396\n",
      "Epoch 277/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0149 - val_loss: 0.0100 - val_mae: 0.0393\n",
      "Epoch 278/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0019 - mae: 0.0195 - val_loss: 0.0110 - val_mae: 0.0464\n",
      "Epoch 279/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0171 - val_loss: 0.0093 - val_mae: 0.0377\n",
      "Epoch 280/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0013 - mae: 0.0162 - val_loss: 0.0099 - val_mae: 0.0383\n",
      "Epoch 281/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0162 - val_loss: 0.0111 - val_mae: 0.0429\n",
      "Epoch 282/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0159 - val_loss: 0.0096 - val_mae: 0.0370\n",
      "Epoch 283/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0013 - mae: 0.0161 - val_loss: 0.0098 - val_mae: 0.0377\n",
      "Epoch 284/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0152 - val_loss: 0.0097 - val_mae: 0.0391\n",
      "Epoch 285/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0174 - val_loss: 0.0099 - val_mae: 0.0386\n",
      "Epoch 286/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0014 - mae: 0.0164 - val_loss: 0.0094 - val_mae: 0.0379\n",
      "Epoch 287/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0015 - mae: 0.0171 - val_loss: 0.0096 - val_mae: 0.0393\n",
      "Epoch 288/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0162 - val_loss: 0.0095 - val_mae: 0.0376\n",
      "Epoch 289/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0024 - mae: 0.0162 - val_loss: 0.0105 - val_mae: 0.0466\n",
      "Epoch 290/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0194 - val_loss: 0.0101 - val_mae: 0.0409\n",
      "Epoch 291/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0169 - val_loss: 0.0095 - val_mae: 0.0393\n",
      "Epoch 292/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0012 - mae: 0.0157 - val_loss: 0.0097 - val_mae: 0.0387\n",
      "Epoch 293/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0015 - mae: 0.0165 - val_loss: 0.0101 - val_mae: 0.0437\n",
      "Epoch 294/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0015 - mae: 0.0169 - val_loss: 0.0103 - val_mae: 0.0397\n",
      "Epoch 295/500\n",
      "18810/18810 [==============================] - 1s 72us/sample - loss: 0.0014 - mae: 0.0165 - val_loss: 0.0098 - val_mae: 0.0382\n",
      "Epoch 296/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0014 - mae: 0.0159 - val_loss: 0.0099 - val_mae: 0.0398\n",
      "Epoch 297/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0164 - val_loss: 0.0110 - val_mae: 0.0395\n",
      "Epoch 298/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0013 - mae: 0.0158 - val_loss: 0.0106 - val_mae: 0.0393\n",
      "Epoch 299/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0011 - mae: 0.0141 - val_loss: 0.0103 - val_mae: 0.0385\n",
      "Epoch 300/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0141 - val_loss: 0.0101 - val_mae: 0.0402\n",
      "Epoch 301/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0012 - mae: 0.0151 - val_loss: 0.0097 - val_mae: 0.0389\n",
      "Epoch 302/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0154 - val_loss: 0.0099 - val_mae: 0.0373\n",
      "Epoch 303/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0010 - mae: 0.0138 - val_loss: 0.0092 - val_mae: 0.0355\n",
      "Epoch 304/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 9.5874e-04 - mae: 0.0131 - val_loss: 0.0092 - val_mae: 0.0346\n",
      "Epoch 305/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0011 - mae: 0.0136 - val_loss: 0.0096 - val_mae: 0.0365\n",
      "Epoch 306/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0154 - val_loss: 0.0098 - val_mae: 0.0370\n",
      "Epoch 307/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0011 - mae: 0.0138 - val_loss: 0.0095 - val_mae: 0.0366\n",
      "Epoch 308/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0013 - mae: 0.0148 - val_loss: 0.0101 - val_mae: 0.0389\n",
      "Epoch 309/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0016 - mae: 0.0173 - val_loss: 0.0104 - val_mae: 0.0391\n",
      "Epoch 310/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0014 - mae: 0.0162 - val_loss: 0.0104 - val_mae: 0.0389\n",
      "Epoch 311/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0013 - mae: 0.0153 - val_loss: 0.0093 - val_mae: 0.0376\n",
      "Epoch 312/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0176 - val_loss: 0.0098 - val_mae: 0.0378\n",
      "Epoch 313/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0160 - val_loss: 0.0106 - val_mae: 0.0399\n",
      "Epoch 314/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0021 - mae: 0.0194 - val_loss: 0.0120 - val_mae: 0.0460\n",
      "Epoch 315/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0026 - mae: 0.0224 - val_loss: 0.0102 - val_mae: 0.0429\n",
      "Epoch 316/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0015 - mae: 0.0175 - val_loss: 0.0103 - val_mae: 0.0415\n",
      "Epoch 317/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0164 - val_loss: 0.0096 - val_mae: 0.0387\n",
      "Epoch 318/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0020 - mae: 0.0205 - val_loss: 0.0099 - val_mae: 0.0397\n",
      "Epoch 319/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0016 - mae: 0.0180 - val_loss: 0.0099 - val_mae: 0.0411\n",
      "Epoch 320/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0152 - val_loss: 0.0095 - val_mae: 0.0404\n",
      "Epoch 321/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0012 - mae: 0.0145 - val_loss: 0.0103 - val_mae: 0.0402\n",
      "Epoch 322/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0147 - val_loss: 0.0096 - val_mae: 0.0375\n",
      "Epoch 323/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0013 - mae: 0.0156 - val_loss: 0.0097 - val_mae: 0.0393\n",
      "Epoch 324/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0013 - mae: 0.0154 - val_loss: 0.0094 - val_mae: 0.0378\n",
      "Epoch 325/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0010 - mae: 0.0141 - val_loss: 0.0089 - val_mae: 0.0378\n",
      "Epoch 326/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0139 - val_loss: 0.0089 - val_mae: 0.0366\n",
      "Epoch 327/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0010 - mae: 0.0132 - val_loss: 0.0086 - val_mae: 0.0348\n",
      "Epoch 328/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 9.6139e-04 - mae: 0.0133 - val_loss: 0.0086 - val_mae: 0.0358\n",
      "Epoch 329/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0012 - mae: 0.0145 - val_loss: 0.0091 - val_mae: 0.0382\n",
      "Epoch 330/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0013 - mae: 0.0151 - val_loss: 0.0095 - val_mae: 0.0381\n",
      "Epoch 331/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0013 - mae: 0.0156 - val_loss: 0.0092 - val_mae: 0.0370\n",
      "Epoch 332/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0013 - mae: 0.0148 - val_loss: 0.0089 - val_mae: 0.0378\n",
      "Epoch 333/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0016 - mae: 0.0167 - val_loss: 0.0097 - val_mae: 0.0403\n",
      "Epoch 334/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0148 - val_loss: 0.0090 - val_mae: 0.0360\n",
      "Epoch 335/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0012 - mae: 0.0147 - val_loss: 0.0088 - val_mae: 0.0356\n",
      "Epoch 336/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 9.6701e-04 - mae: 0.0132 - val_loss: 0.0095 - val_mae: 0.0359\n",
      "Epoch 337/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0011 - mae: 0.0137 - val_loss: 0.0093 - val_mae: 0.0369\n",
      "Epoch 338/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0013 - mae: 0.0159 - val_loss: 0.0096 - val_mae: 0.0378\n",
      "Epoch 339/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0014 - mae: 0.0151 - val_loss: 0.0100 - val_mae: 0.0404\n",
      "Epoch 340/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0015 - mae: 0.0159 - val_loss: 0.0104 - val_mae: 0.0411\n",
      "Epoch 341/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0013 - mae: 0.0157 - val_loss: 0.0098 - val_mae: 0.0390\n",
      "Epoch 342/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0011 - mae: 0.0138 - val_loss: 0.0089 - val_mae: 0.0384\n",
      "Epoch 343/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0014 - mae: 0.0157 - val_loss: 0.0098 - val_mae: 0.0378\n",
      "Epoch 344/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0016 - mae: 0.0160 - val_loss: 0.0090 - val_mae: 0.0372\n",
      "Epoch 345/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0016 - mae: 0.0162 - val_loss: 0.0100 - val_mae: 0.0406\n",
      "Epoch 346/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0179 - val_loss: 0.0096 - val_mae: 0.0381\n",
      "Epoch 347/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0160 - val_loss: 0.0093 - val_mae: 0.0382\n",
      "Epoch 348/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0016 - mae: 0.0170 - val_loss: 0.0096 - val_mae: 0.0396\n",
      "Epoch 349/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0156 - val_loss: 0.0097 - val_mae: 0.0388\n",
      "Epoch 350/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0159 - val_loss: 0.0109 - val_mae: 0.0399\n",
      "Epoch 351/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0021 - mae: 0.0188 - val_loss: 0.0103 - val_mae: 0.0392\n",
      "Epoch 352/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0017 - mae: 0.0174 - val_loss: 0.0093 - val_mae: 0.0377\n",
      "Epoch 353/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0014 - mae: 0.0157 - val_loss: 0.0090 - val_mae: 0.0366\n",
      "Epoch 354/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0013 - mae: 0.0158 - val_loss: 0.0090 - val_mae: 0.0381\n",
      "Epoch 355/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0014 - mae: 0.0155 - val_loss: 0.0089 - val_mae: 0.0366\n",
      "Epoch 356/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0011 - mae: 0.0138 - val_loss: 0.0102 - val_mae: 0.0380\n",
      "Epoch 357/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0015 - mae: 0.0158 - val_loss: 0.0097 - val_mae: 0.0411\n",
      "Epoch 358/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0012 - mae: 0.0152 - val_loss: 0.0100 - val_mae: 0.0376\n",
      "Epoch 359/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0011 - mae: 0.0143 - val_loss: 0.0091 - val_mae: 0.0370\n",
      "Epoch 360/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 9.9220e-04 - mae: 0.0130 - val_loss: 0.0090 - val_mae: 0.0349\n",
      "Epoch 361/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0012 - mae: 0.0141 - val_loss: 0.0096 - val_mae: 0.0390\n",
      "Epoch 362/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0018 - mae: 0.0188 - val_loss: 0.0103 - val_mae: 0.0407\n",
      "Epoch 363/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0016 - mae: 0.0167 - val_loss: 0.0110 - val_mae: 0.0409\n",
      "Epoch 364/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0015 - mae: 0.0161 - val_loss: 0.0096 - val_mae: 0.0375\n",
      "Epoch 365/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0016 - mae: 0.0163 - val_loss: 0.0107 - val_mae: 0.0402\n",
      "Epoch 366/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0023 - mae: 0.0201 - val_loss: 0.0115 - val_mae: 0.0460\n",
      "Epoch 367/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0022 - mae: 0.0206 - val_loss: 0.0104 - val_mae: 0.0401\n",
      "Epoch 368/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0016 - mae: 0.0169 - val_loss: 0.0100 - val_mae: 0.0395\n",
      "Epoch 369/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0160 - val_loss: 0.0099 - val_mae: 0.0366\n",
      "Epoch 370/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0157 - val_loss: 0.0100 - val_mae: 0.0394\n",
      "Epoch 371/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0173 - val_loss: 0.0107 - val_mae: 0.0392\n",
      "Epoch 372/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0021 - mae: 0.0187 - val_loss: 0.0111 - val_mae: 0.0410\n",
      "Epoch 373/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0175 - val_loss: 0.0109 - val_mae: 0.0415\n",
      "Epoch 374/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0177 - val_loss: 0.0093 - val_mae: 0.0374\n",
      "Epoch 375/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0175 - val_loss: 0.0100 - val_mae: 0.0397\n",
      "Epoch 376/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0012 - mae: 0.0150 - val_loss: 0.0097 - val_mae: 0.0376\n",
      "Epoch 377/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0013 - mae: 0.0151 - val_loss: 0.0093 - val_mae: 0.0370\n",
      "Epoch 378/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0011 - mae: 0.0142 - val_loss: 0.0092 - val_mae: 0.0368\n",
      "Epoch 379/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0011 - mae: 0.0142 - val_loss: 0.0094 - val_mae: 0.0366\n",
      "Epoch 380/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0150 - val_loss: 0.0101 - val_mae: 0.0390\n",
      "Epoch 381/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0137 - val_loss: 0.0090 - val_mae: 0.0344\n",
      "Epoch 382/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0136 - val_loss: 0.0094 - val_mae: 0.0368\n",
      "Epoch 383/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0015 - mae: 0.0156 - val_loss: 0.0093 - val_mae: 0.0374\n",
      "Epoch 384/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0161 - val_loss: 0.0102 - val_mae: 0.0399\n",
      "Epoch 385/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0012 - mae: 0.0152 - val_loss: 0.0091 - val_mae: 0.0376\n",
      "Epoch 386/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 9.8417e-04 - mae: 0.0134 - val_loss: 0.0087 - val_mae: 0.0351\n",
      "Epoch 387/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0017 - mae: 0.0164 - val_loss: 0.0108 - val_mae: 0.0430\n",
      "Epoch 388/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0177 - val_loss: 0.0097 - val_mae: 0.0389\n",
      "Epoch 389/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0146 - val_loss: 0.0087 - val_mae: 0.0369\n",
      "Epoch 390/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 9.7259e-04 - mae: 0.0132 - val_loss: 0.0098 - val_mae: 0.0377\n",
      "Epoch 391/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0016 - mae: 0.0158 - val_loss: 0.0098 - val_mae: 0.0390\n",
      "Epoch 392/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0017 - mae: 0.0176 - val_loss: 0.0109 - val_mae: 0.0408\n",
      "Epoch 393/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0019 - mae: 0.0187 - val_loss: 0.0101 - val_mae: 0.0386\n",
      "Epoch 394/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0013 - mae: 0.0157 - val_loss: 0.0104 - val_mae: 0.0381\n",
      "Epoch 395/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0145 - val_loss: 0.0107 - val_mae: 0.0388\n",
      "Epoch 396/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0013 - mae: 0.0150 - val_loss: 0.0094 - val_mae: 0.0377\n",
      "Epoch 397/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0010 - mae: 0.0133 - val_loss: 0.0092 - val_mae: 0.0351\n",
      "Epoch 398/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0011 - mae: 0.0144 - val_loss: 0.0098 - val_mae: 0.0358\n",
      "Epoch 399/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0014 - mae: 0.0147 - val_loss: 0.0094 - val_mae: 0.0374\n",
      "Epoch 400/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0013 - mae: 0.0155 - val_loss: 0.0093 - val_mae: 0.0408\n",
      "Epoch 401/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0012 - mae: 0.0148 - val_loss: 0.0095 - val_mae: 0.0371\n",
      "Epoch 402/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0153 - val_loss: 0.0097 - val_mae: 0.0393\n",
      "Epoch 403/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0012 - mae: 0.0152 - val_loss: 0.0097 - val_mae: 0.0369\n",
      "Epoch 404/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0011 - mae: 0.0136 - val_loss: 0.0091 - val_mae: 0.0362\n",
      "Epoch 405/500\n",
      "18810/18810 [==============================] - 1s 80us/sample - loss: 0.0011 - mae: 0.0138 - val_loss: 0.0102 - val_mae: 0.0367\n",
      "Epoch 406/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0012 - mae: 0.0147 - val_loss: 0.0097 - val_mae: 0.0381\n",
      "Epoch 407/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0016 - mae: 0.0161 - val_loss: 0.0106 - val_mae: 0.0431\n",
      "Epoch 408/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0023 - mae: 0.0203 - val_loss: 0.0115 - val_mae: 0.0449\n",
      "Epoch 409/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0019 - mae: 0.0185 - val_loss: 0.0092 - val_mae: 0.0373\n",
      "Epoch 410/500\n",
      "18810/18810 [==============================] - 2s 83us/sample - loss: 0.0013 - mae: 0.0154 - val_loss: 0.0103 - val_mae: 0.0400\n",
      "Epoch 411/500\n",
      "18810/18810 [==============================] - 1s 80us/sample - loss: 0.0014 - mae: 0.0154 - val_loss: 0.0099 - val_mae: 0.0378\n",
      "Epoch 412/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0015 - mae: 0.0163 - val_loss: 0.0103 - val_mae: 0.0392\n",
      "Epoch 413/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 9.9554e-04 - mae: 0.0139 - val_loss: 0.0093 - val_mae: 0.0344\n",
      "Epoch 414/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 9.7029e-04 - mae: 0.0126 - val_loss: 0.0094 - val_mae: 0.0356\n",
      "Epoch 415/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0010 - mae: 0.0128 - val_loss: 0.0092 - val_mae: 0.0350\n",
      "Epoch 416/500\n",
      "18810/18810 [==============================] - 2s 86us/sample - loss: 0.0012 - mae: 0.0137 - val_loss: 0.0106 - val_mae: 0.0398\n",
      "Epoch 417/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0015 - mae: 0.0162 - val_loss: 0.0093 - val_mae: 0.0367\n",
      "Epoch 418/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0013 - mae: 0.0150 - val_loss: 0.0097 - val_mae: 0.0375\n",
      "Epoch 419/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0014 - mae: 0.0153 - val_loss: 0.0094 - val_mae: 0.0396\n",
      "Epoch 420/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0015 - mae: 0.0162 - val_loss: 0.0105 - val_mae: 0.0407\n",
      "Epoch 421/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0012 - mae: 0.0150 - val_loss: 0.0096 - val_mae: 0.0385\n",
      "Epoch 422/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0011 - mae: 0.0137 - val_loss: 0.0094 - val_mae: 0.0352\n",
      "Epoch 423/500\n",
      "18810/18810 [==============================] - 2s 99us/sample - loss: 9.2800e-04 - mae: 0.0129 - val_loss: 0.0096 - val_mae: 0.0363\n",
      "Epoch 424/500\n",
      "18810/18810 [==============================] - 2s 95us/sample - loss: 8.7361e-04 - mae: 0.0123 - val_loss: 0.0092 - val_mae: 0.0350\n",
      "Epoch 425/500\n",
      "18810/18810 [==============================] - 1s 76us/sample - loss: 0.0012 - mae: 0.0133 - val_loss: 0.0102 - val_mae: 0.0371\n",
      "Epoch 426/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0012 - mae: 0.0145 - val_loss: 0.0101 - val_mae: 0.0391\n",
      "Epoch 427/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0011 - mae: 0.0135 - val_loss: 0.0093 - val_mae: 0.0344\n",
      "Epoch 428/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0014 - mae: 0.0144 - val_loss: 0.0091 - val_mae: 0.0368\n",
      "Epoch 429/500\n",
      "18810/18810 [==============================] - 2s 83us/sample - loss: 0.0011 - mae: 0.0129 - val_loss: 0.0098 - val_mae: 0.0380\n",
      "Epoch 430/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0013 - mae: 0.0149 - val_loss: 0.0085 - val_mae: 0.0346\n",
      "Epoch 431/500\n",
      "18810/18810 [==============================] - 2s 80us/sample - loss: 0.0017 - mae: 0.0159 - val_loss: 0.0104 - val_mae: 0.0429\n",
      "Epoch 432/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0017 - mae: 0.0170 - val_loss: 0.0092 - val_mae: 0.0376\n",
      "Epoch 433/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0015 - mae: 0.0160 - val_loss: 0.0105 - val_mae: 0.0411\n",
      "Epoch 434/500\n",
      "18810/18810 [==============================] - 2s 81us/sample - loss: 0.0015 - mae: 0.0151 - val_loss: 0.0085 - val_mae: 0.0333\n",
      "Epoch 435/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0020 - mae: 0.0175 - val_loss: 0.0097 - val_mae: 0.0394\n",
      "Epoch 436/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0013 - mae: 0.0151 - val_loss: 0.0093 - val_mae: 0.0351\n",
      "Epoch 437/500\n",
      "18810/18810 [==============================] - 1s 80us/sample - loss: 0.0011 - mae: 0.0132 - val_loss: 0.0097 - val_mae: 0.0355\n",
      "Epoch 438/500\n",
      "18810/18810 [==============================] - 2s 84us/sample - loss: 0.0011 - mae: 0.0126 - val_loss: 0.0094 - val_mae: 0.0354\n",
      "Epoch 439/500\n",
      "18810/18810 [==============================] - 2s 82us/sample - loss: 0.0011 - mae: 0.0127 - val_loss: 0.0097 - val_mae: 0.0365\n",
      "Epoch 440/500\n",
      "18810/18810 [==============================] - 1s 77us/sample - loss: 0.0010 - mae: 0.0137 - val_loss: 0.0098 - val_mae: 0.0378\n",
      "Epoch 441/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0014 - mae: 0.0151 - val_loss: 0.0095 - val_mae: 0.0367\n",
      "Epoch 442/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0012 - mae: 0.0148 - val_loss: 0.0102 - val_mae: 0.0384\n",
      "Epoch 443/500\n",
      "18810/18810 [==============================] - 2s 84us/sample - loss: 0.0010 - mae: 0.0130 - val_loss: 0.0099 - val_mae: 0.0366\n",
      "Epoch 444/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0011 - mae: 0.0130 - val_loss: 0.0098 - val_mae: 0.0371\n",
      "Epoch 445/500\n",
      "18810/18810 [==============================] - 1s 79us/sample - loss: 0.0012 - mae: 0.0138 - val_loss: 0.0093 - val_mae: 0.0353\n",
      "Epoch 446/500\n",
      "18810/18810 [==============================] - 1s 78us/sample - loss: 0.0011 - mae: 0.0126 - val_loss: 0.0096 - val_mae: 0.0375\n",
      "Epoch 447/500\n",
      "18810/18810 [==============================] - 1s 76us/sample - loss: 0.0013 - mae: 0.0146 - val_loss: 0.0105 - val_mae: 0.0392\n",
      "Epoch 448/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0012 - mae: 0.0142 - val_loss: 0.0093 - val_mae: 0.0358\n",
      "Epoch 449/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0010 - mae: 0.0127 - val_loss: 0.0093 - val_mae: 0.0347\n",
      "Epoch 450/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 9.9223e-04 - mae: 0.0123 - val_loss: 0.0105 - val_mae: 0.0370\n",
      "Epoch 451/500\n",
      "18810/18810 [==============================] - 1s 76us/sample - loss: 0.0011 - mae: 0.0124 - val_loss: 0.0098 - val_mae: 0.0353\n",
      "Epoch 452/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0012 - mae: 0.0134 - val_loss: 0.0102 - val_mae: 0.0375\n",
      "Epoch 453/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 8.9932e-04 - mae: 0.0128 - val_loss: 0.0108 - val_mae: 0.0375\n",
      "Epoch 454/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 8.2264e-04 - mae: 0.0111 - val_loss: 0.0105 - val_mae: 0.0382\n",
      "Epoch 455/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 8.9641e-04 - mae: 0.0121 - val_loss: 0.0098 - val_mae: 0.0350\n",
      "Epoch 456/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 8.9544e-04 - mae: 0.0117 - val_loss: 0.0098 - val_mae: 0.0350\n",
      "Epoch 457/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0012 - mae: 0.0131 - val_loss: 0.0107 - val_mae: 0.0379\n",
      "Epoch 458/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0011 - mae: 0.0130 - val_loss: 0.0098 - val_mae: 0.0350\n",
      "Epoch 459/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 9.8118e-04 - mae: 0.0129 - val_loss: 0.0094 - val_mae: 0.0359\n",
      "Epoch 460/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0010 - mae: 0.0120 - val_loss: 0.0093 - val_mae: 0.0357\n",
      "Epoch 461/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0015 - mae: 0.0150 - val_loss: 0.0106 - val_mae: 0.0412\n",
      "Epoch 462/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0025 - mae: 0.0194 - val_loss: 0.0123 - val_mae: 0.0463\n",
      "Epoch 463/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0022 - mae: 0.0195 - val_loss: 0.0101 - val_mae: 0.0398\n",
      "Epoch 464/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0013 - mae: 0.0144 - val_loss: 0.0091 - val_mae: 0.0359\n",
      "Epoch 465/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0013 - mae: 0.0151 - val_loss: 0.0097 - val_mae: 0.0389\n",
      "Epoch 466/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0014 - mae: 0.0153 - val_loss: 0.0102 - val_mae: 0.0393\n",
      "Epoch 467/500\n",
      "18810/18810 [==============================] - 1s 63us/sample - loss: 0.0014 - mae: 0.0153 - val_loss: 0.0114 - val_mae: 0.0427\n",
      "Epoch 468/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0169 - val_loss: 0.0098 - val_mae: 0.0402\n",
      "Epoch 469/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0158 - val_loss: 0.0098 - val_mae: 0.0367\n",
      "Epoch 470/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0012 - mae: 0.0144 - val_loss: 0.0099 - val_mae: 0.0367\n",
      "Epoch 471/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0013 - mae: 0.0142 - val_loss: 0.0096 - val_mae: 0.0398\n",
      "Epoch 472/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0142 - val_loss: 0.0108 - val_mae: 0.0413\n",
      "Epoch 473/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0016 - mae: 0.0154 - val_loss: 0.0095 - val_mae: 0.0378\n",
      "Epoch 474/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0011 - mae: 0.0130 - val_loss: 0.0101 - val_mae: 0.0368\n",
      "Epoch 475/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 9.5519e-04 - mae: 0.0124 - val_loss: 0.0092 - val_mae: 0.0346\n",
      "Epoch 476/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 8.1923e-04 - mae: 0.0113 - val_loss: 0.0088 - val_mae: 0.0342\n",
      "Epoch 477/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 7.9893e-04 - mae: 0.0117 - val_loss: 0.0088 - val_mae: 0.0339\n",
      "Epoch 478/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0012 - mae: 0.0137 - val_loss: 0.0091 - val_mae: 0.0362\n",
      "Epoch 479/500\n",
      "18810/18810 [==============================] - 1s 65us/sample - loss: 0.0010 - mae: 0.0128 - val_loss: 0.0093 - val_mae: 0.0352\n",
      "Epoch 480/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0010 - mae: 0.0127 - val_loss: 0.0097 - val_mae: 0.0358\n",
      "Epoch 481/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 8.4878e-04 - mae: 0.0119 - val_loss: 0.0100 - val_mae: 0.0368\n",
      "Epoch 482/500\n",
      "18810/18810 [==============================] - 2s 83us/sample - loss: 0.0017 - mae: 0.0159 - val_loss: 0.0098 - val_mae: 0.0371\n",
      "Epoch 483/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0014 - mae: 0.0147 - val_loss: 0.0094 - val_mae: 0.0359\n",
      "Epoch 484/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0013 - mae: 0.0147 - val_loss: 0.0097 - val_mae: 0.0367\n",
      "Epoch 485/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0019 - mae: 0.0166 - val_loss: 0.0108 - val_mae: 0.0435\n",
      "Epoch 486/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0019 - mae: 0.0190 - val_loss: 0.0103 - val_mae: 0.0393\n",
      "Epoch 487/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0017 - mae: 0.0167 - val_loss: 0.0105 - val_mae: 0.0397\n",
      "Epoch 488/500\n",
      "18810/18810 [==============================] - 1s 64us/sample - loss: 0.0014 - mae: 0.0161 - val_loss: 0.0096 - val_mae: 0.0375\n",
      "Epoch 489/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0014 - mae: 0.0151 - val_loss: 0.0102 - val_mae: 0.0420\n",
      "Epoch 490/500\n",
      "18810/18810 [==============================] - 1s 66us/sample - loss: 0.0015 - mae: 0.0161 - val_loss: 0.0100 - val_mae: 0.0396\n",
      "Epoch 491/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0017 - mae: 0.0161 - val_loss: 0.0096 - val_mae: 0.0373\n",
      "Epoch 492/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0012 - mae: 0.0144 - val_loss: 0.0108 - val_mae: 0.0375\n",
      "Epoch 493/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0013 - mae: 0.0142 - val_loss: 0.0120 - val_mae: 0.0444\n",
      "Epoch 494/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0017 - mae: 0.0161 - val_loss: 0.0095 - val_mae: 0.0365\n",
      "Epoch 495/500\n",
      "18810/18810 [==============================] - 1s 70us/sample - loss: 0.0015 - mae: 0.0153 - val_loss: 0.0095 - val_mae: 0.0400\n",
      "Epoch 496/500\n",
      "18810/18810 [==============================] - 1s 73us/sample - loss: 0.0014 - mae: 0.0150 - val_loss: 0.0090 - val_mae: 0.0351\n",
      "Epoch 497/500\n",
      "18810/18810 [==============================] - 1s 69us/sample - loss: 0.0012 - mae: 0.0147 - val_loss: 0.0093 - val_mae: 0.0359\n",
      "Epoch 498/500\n",
      "18810/18810 [==============================] - 1s 71us/sample - loss: 0.0010 - mae: 0.0132 - val_loss: 0.0094 - val_mae: 0.0353\n",
      "Epoch 499/500\n",
      "18810/18810 [==============================] - 1s 67us/sample - loss: 0.0013 - mae: 0.0155 - val_loss: 0.0092 - val_mae: 0.0352\n",
      "Epoch 500/500\n",
      "18810/18810 [==============================] - 1s 68us/sample - loss: 0.0012 - mae: 0.0134 - val_loss: 0.0095 - val_mae: 0.0356\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "Adam=Adam(lr=0.00001)\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['mae'])\n",
    "checkpoint_save_path = \"./checkpoint/No20240321.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('- - - - - - - load the model- - - - - - -')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True)\n",
    "history = model.fit(X_train, Y_train, epochs=500, validation_data=(X_test, Y_test), batch_size=209, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.06363666301800144,\n",
       "  0.032782297643522425,\n",
       "  0.024830933122171297,\n",
       "  0.02211057890413536,\n",
       "  0.019109037021795908,\n",
       "  0.017208716614792743,\n",
       "  0.01660662918455071,\n",
       "  0.015241123497900036,\n",
       "  0.013819609768688679,\n",
       "  0.013053658915062745,\n",
       "  0.012154139128203194,\n",
       "  0.011888473574072123,\n",
       "  0.011084735833315386,\n",
       "  0.010785024463095599,\n",
       "  0.009902250616707735,\n",
       "  0.009698417037725449,\n",
       "  0.009022797256087263,\n",
       "  0.008623928675014112,\n",
       "  0.008825496832529704,\n",
       "  0.008124316659652525,\n",
       "  0.007645147361068262,\n",
       "  0.008398333238437772,\n",
       "  0.007149504621823629,\n",
       "  0.006950228115440243,\n",
       "  0.006921132348684801,\n",
       "  0.007083695966543423,\n",
       "  0.006814140278018183,\n",
       "  0.00669567225397461,\n",
       "  0.00652570896781981,\n",
       "  0.006204439425427053,\n",
       "  0.006273812334984541,\n",
       "  0.005729718180373311,\n",
       "  0.0052276330405018395,\n",
       "  0.005233017494902015,\n",
       "  0.005385148579565188,\n",
       "  0.005195663289891349,\n",
       "  0.004997796197939251,\n",
       "  0.005193755366942949,\n",
       "  0.006202034494425687,\n",
       "  0.005313784874872201,\n",
       "  0.004981308984052804,\n",
       "  0.005237937972156538,\n",
       "  0.005244695889349613,\n",
       "  0.004670355810473362,\n",
       "  0.004584393417462707,\n",
       "  0.0053011299835311046,\n",
       "  0.004735415581510299,\n",
       "  0.004703625058755279,\n",
       "  0.004283206493386792,\n",
       "  0.0042142457328736786,\n",
       "  0.0043224169588130384,\n",
       "  0.004591594957229164,\n",
       "  0.004348219708643026,\n",
       "  0.00451486077832265,\n",
       "  0.005099631225069364,\n",
       "  0.004324100634807514,\n",
       "  0.004272990835468388,\n",
       "  0.0045461820697204934,\n",
       "  0.004269722887935738,\n",
       "  0.0038766610410271417,\n",
       "  0.0035678220266062355,\n",
       "  0.0034181910450570285,\n",
       "  0.0034754802227123746,\n",
       "  0.0033641922016007203,\n",
       "  0.0033625880799566707,\n",
       "  0.003818835058094313,\n",
       "  0.0036946291621360516,\n",
       "  0.0032891585685623187,\n",
       "  0.002968536967980779,\n",
       "  0.0030394074830433562,\n",
       "  0.003245731937285099,\n",
       "  0.0033446424511364764,\n",
       "  0.003476440499071032,\n",
       "  0.0030127226982990074,\n",
       "  0.003381584919730408,\n",
       "  0.004524691715940005,\n",
       "  0.004561973060481251,\n",
       "  0.0038902840990987088,\n",
       "  0.003014109610942089,\n",
       "  0.0030940327120737897,\n",
       "  0.0033013940336079233,\n",
       "  0.0030186456214222643,\n",
       "  0.0030553062937946785,\n",
       "  0.0029767500521201227,\n",
       "  0.0035575447254814208,\n",
       "  0.003174727680420296,\n",
       "  0.002845097725124409,\n",
       "  0.0029368073938207495,\n",
       "  0.004466957915833013,\n",
       "  0.002846421795483265,\n",
       "  0.0027243869611993433,\n",
       "  0.0027548655544200706,\n",
       "  0.0024570118040881223,\n",
       "  0.004089846158037997,\n",
       "  0.0031893670895240373,\n",
       "  0.0032684286073264147,\n",
       "  0.002757435950398859,\n",
       "  0.0026288528124698333,\n",
       "  0.0031792254316517048,\n",
       "  0.0032317542005330323,\n",
       "  0.0030469428178750806,\n",
       "  0.002610133458963699,\n",
       "  0.0030989378781264855,\n",
       "  0.0028918124631875093,\n",
       "  0.002919810458034691,\n",
       "  0.0025555178471323515,\n",
       "  0.0025229608252023655,\n",
       "  0.002223741805129167,\n",
       "  0.002015266634730829,\n",
       "  0.003258919929309438,\n",
       "  0.0026580101564630037,\n",
       "  0.00239991292085809,\n",
       "  0.002726801720240878,\n",
       "  0.0029699589487993056,\n",
       "  0.0028581133771998185,\n",
       "  0.002660716318577114,\n",
       "  0.0022450670561132333,\n",
       "  0.002449788666692459,\n",
       "  0.0025990954044068024,\n",
       "  0.0028792742639780046,\n",
       "  0.0035106296974441245,\n",
       "  0.003041838757538547,\n",
       "  0.0026258383010928,\n",
       "  0.002354581751084576,\n",
       "  0.002376636733404464,\n",
       "  0.0026452740473258828,\n",
       "  0.0025982072874386275,\n",
       "  0.002956464962982055,\n",
       "  0.002540259629798432,\n",
       "  0.002257937000184837,\n",
       "  0.002010850857995037,\n",
       "  0.002165935259674572,\n",
       "  0.0019628838597175974,\n",
       "  0.002194067277661007,\n",
       "  0.0019172536429121262,\n",
       "  0.0018588107370305807,\n",
       "  0.0019179052712085347,\n",
       "  0.002086633563481478,\n",
       "  0.002323276528881656,\n",
       "  0.0018309931885192377,\n",
       "  0.0017066063087744017,\n",
       "  0.002895393434705006,\n",
       "  0.0025879215656055346,\n",
       "  0.002025846483755029,\n",
       "  0.002055734181259241,\n",
       "  0.0022245261236093937,\n",
       "  0.00199164469118437,\n",
       "  0.0021805898719725924,\n",
       "  0.002231662420348989,\n",
       "  0.0020973534556105733,\n",
       "  0.002118210327009567,\n",
       "  0.002081884932057518,\n",
       "  0.002073293912690133,\n",
       "  0.002204311001373248,\n",
       "  0.002071925524958513,\n",
       "  0.0023029279565283407,\n",
       "  0.002293020017289867,\n",
       "  0.001918745286659234,\n",
       "  0.0020167272279245987,\n",
       "  0.0017825129648877515,\n",
       "  0.0021633783200134832,\n",
       "  0.0016815468216211432,\n",
       "  0.001802426072794737,\n",
       "  0.0017163854338125223,\n",
       "  0.0019226653527261483,\n",
       "  0.0017642900470592495,\n",
       "  0.0018314505487473476,\n",
       "  0.0019753086445335713,\n",
       "  0.0018046632784211802,\n",
       "  0.0018356395995296124,\n",
       "  0.0018668735128206512,\n",
       "  0.001670290419780132,\n",
       "  0.001818071109139257,\n",
       "  0.0019337098663931506,\n",
       "  0.0019134722468960616,\n",
       "  0.0024195850975552782,\n",
       "  0.002169363884927912,\n",
       "  0.001817344405895306,\n",
       "  0.0017501478064029167,\n",
       "  0.001654635081326382,\n",
       "  0.001955387857095856,\n",
       "  0.0020662644903899894,\n",
       "  0.001444506256504812,\n",
       "  0.001385866126575921,\n",
       "  0.0015376347912630687,\n",
       "  0.001724634446307189,\n",
       "  0.0024687163589987905,\n",
       "  0.0022777289889442426,\n",
       "  0.0017250500541801253,\n",
       "  0.0021214382587155948,\n",
       "  0.0019724294329838206,\n",
       "  0.0016231157428895433,\n",
       "  0.001703216860510616,\n",
       "  0.0016002258939099394,\n",
       "  0.0017475639541064286,\n",
       "  0.0018321939368939233,\n",
       "  0.0019512545323878941,\n",
       "  0.002116893810711594,\n",
       "  0.0020596359972842038,\n",
       "  0.0015828566826207357,\n",
       "  0.0016493744501430128,\n",
       "  0.0016130154629030989,\n",
       "  0.0017739091501829938,\n",
       "  0.002343859976907778,\n",
       "  0.0017639012315258799,\n",
       "  0.0014610411592810933,\n",
       "  0.0014382505065037145,\n",
       "  0.0014175556179705179,\n",
       "  0.001649473228543583,\n",
       "  0.0014267497113905846,\n",
       "  0.0013772053204269873,\n",
       "  0.0013255798532756874,\n",
       "  0.0013035303906589332,\n",
       "  0.0012491373008944923,\n",
       "  0.0015965408561492546,\n",
       "  0.0016231265572261894,\n",
       "  0.0017534692590642307,\n",
       "  0.0018993650724749185,\n",
       "  0.00204892604249633,\n",
       "  0.0016625303451696205,\n",
       "  0.001529182130858923,\n",
       "  0.0015815054473932832,\n",
       "  0.001304632966639474,\n",
       "  0.0013807942185343968,\n",
       "  0.0015099907979472644,\n",
       "  0.0018448893423838955,\n",
       "  0.0014745935881769077,\n",
       "  0.0017708470592171781,\n",
       "  0.0022090947177881995,\n",
       "  0.0022837726366964893,\n",
       "  0.0018529866388740225,\n",
       "  0.0018963372871641897,\n",
       "  0.001562792969505406,\n",
       "  0.0014996512980562531,\n",
       "  0.0013556920603150502,\n",
       "  0.0014413967220914654,\n",
       "  0.0014660889554458359,\n",
       "  0.0011845106802259883,\n",
       "  0.0011668304599071336,\n",
       "  0.001020434835421232,\n",
       "  0.0012022421310474683,\n",
       "  0.0014050779366193132,\n",
       "  0.0015018713628200608,\n",
       "  0.0015939697817278404,\n",
       "  0.0024141948165682455,\n",
       "  0.00194841703826872,\n",
       "  0.0019062503040509506,\n",
       "  0.0017500250981861932,\n",
       "  0.0014241285822612958,\n",
       "  0.0015229675957622627,\n",
       "  0.0015138788019410437,\n",
       "  0.00131901062041935,\n",
       "  0.001380355431077381,\n",
       "  0.0017961323629909506,\n",
       "  0.0015174623138995635,\n",
       "  0.0012892905849083843,\n",
       "  0.0011226489363859097,\n",
       "  0.0011868448556116265,\n",
       "  0.0012830971669043519,\n",
       "  0.0015111674489970835,\n",
       "  0.0014929597563524213,\n",
       "  0.0013059669911550979,\n",
       "  0.0014511477079294209,\n",
       "  0.0016632559748056034,\n",
       "  0.0017068573700574536,\n",
       "  0.001546038051472149,\n",
       "  0.0014942208809467653,\n",
       "  0.001525423934476243,\n",
       "  0.0013157112692068849,\n",
       "  0.0012770002914799584,\n",
       "  0.0011887798231327907,\n",
       "  0.00099052405599246,\n",
       "  0.0011384493910655794,\n",
       "  0.001454422347726197,\n",
       "  0.0014183657163003873,\n",
       "  0.0011155768100353373,\n",
       "  0.0011399003327824176,\n",
       "  0.0019052572050390559,\n",
       "  0.0014423340222694807,\n",
       "  0.0012743986945780409,\n",
       "  0.0013887976005207748,\n",
       "  0.001365287008553019,\n",
       "  0.0012693665876415455,\n",
       "  0.0012716635098008232,\n",
       "  0.0015139910638228887,\n",
       "  0.0013664686806603439,\n",
       "  0.001524334225622523,\n",
       "  0.001372282513572524,\n",
       "  0.002358943132437869,\n",
       "  0.0017224752635229379,\n",
       "  0.0014017044624779374,\n",
       "  0.0012257033250837898,\n",
       "  0.0014748441960869565,\n",
       "  0.0014755292802066024,\n",
       "  0.0013994881980276357,\n",
       "  0.0013844549018863796,\n",
       "  0.0014209645062995453,\n",
       "  0.0012501146165757543,\n",
       "  0.0010625803804335495,\n",
       "  0.0011816687484194214,\n",
       "  0.0011518391311660203,\n",
       "  0.0012213581662055932,\n",
       "  0.0010096182613374871,\n",
       "  0.0009587401313991803,\n",
       "  0.001134297856090901,\n",
       "  0.0012260282793754918,\n",
       "  0.0011400916914377983,\n",
       "  0.001286076826444413,\n",
       "  0.0015636013723754635,\n",
       "  0.0013920554884761158,\n",
       "  0.0012992784959755632,\n",
       "  0.0016977678437898349,\n",
       "  0.0014136071051729635,\n",
       "  0.002141342242895108,\n",
       "  0.0025933649306858165,\n",
       "  0.001513969721337263,\n",
       "  0.0014365133114754118,\n",
       "  0.002030532910592026,\n",
       "  0.0016157349723572325,\n",
       "  0.0011863392257105765,\n",
       "  0.0011783355361937235,\n",
       "  0.0012630128838484072,\n",
       "  0.0013127478974638506,\n",
       "  0.0012967866227780987,\n",
       "  0.0010358888035019239,\n",
       "  0.0010820096693350933,\n",
       "  0.0010050085285911337,\n",
       "  0.0009613934939908278,\n",
       "  0.0012429768869575734,\n",
       "  0.0012714344503668448,\n",
       "  0.0013232077057990764,\n",
       "  0.0012983340608318232,\n",
       "  0.0015956932801701542,\n",
       "  0.0013860661141936563,\n",
       "  0.0012409410077250666,\n",
       "  0.0009670055959658283,\n",
       "  0.0011208406449036879,\n",
       "  0.0013335242634639143,\n",
       "  0.0014008767023268674,\n",
       "  0.0014754404126304304,\n",
       "  0.001313182617822248,\n",
       "  0.0011232541848181022,\n",
       "  0.0014008573262900528,\n",
       "  0.0015883820545342234,\n",
       "  0.0015732993442927384,\n",
       "  0.001691414154548612,\n",
       "  0.0014764295829485896,\n",
       "  0.0016390440927352757,\n",
       "  0.001264933756384481,\n",
       "  0.0016569878673180938,\n",
       "  0.002118322421382699,\n",
       "  0.0016826250997837634,\n",
       "  0.0014171696183944327,\n",
       "  0.0013427831277820386,\n",
       "  0.0013736220982132686,\n",
       "  0.001052738133714431,\n",
       "  0.0015155225778774668,\n",
       "  0.0011991017100323612,\n",
       "  0.0010954302053303561,\n",
       "  0.0009921955562377763,\n",
       "  0.0011945525743714016,\n",
       "  0.0017624505595045371,\n",
       "  0.0015549870194970733,\n",
       "  0.0014902029635979691,\n",
       "  0.0016148805129988532,\n",
       "  0.0022863030407784713,\n",
       "  0.0022409760296189535,\n",
       "  0.001568207484928684,\n",
       "  0.0013954919177599045,\n",
       "  0.0014675644397761467,\n",
       "  0.0016987361097967045,\n",
       "  0.0020876323613467523,\n",
       "  0.001726353561712636,\n",
       "  0.0017190478489889454,\n",
       "  0.0016960807616770682,\n",
       "  0.0012203230134521922,\n",
       "  0.0012809564592316746,\n",
       "  0.0011031679408107368,\n",
       "  0.0010932510495573903,\n",
       "  0.0011752697710310006,\n",
       "  0.0010720621904410008,\n",
       "  0.001067151156732709,\n",
       "  0.001475835316361756,\n",
       "  0.0016500992131316,\n",
       "  0.0012232754634977836,\n",
       "  0.0009841665024093041,\n",
       "  0.0016522234178005925,\n",
       "  0.0015370399050880224,\n",
       "  0.0012036393467699073,\n",
       "  0.0009725858211620815,\n",
       "  0.00155159030497695,\n",
       "  0.0017100959175473285,\n",
       "  0.001906086776741884,\n",
       "  0.0013326759207605695,\n",
       "  0.0012361573815319895,\n",
       "  0.0013423898448106937,\n",
       "  0.0010046421255942228,\n",
       "  0.0010887225983121122,\n",
       "  0.0013811326262334156,\n",
       "  0.0013052762654196057,\n",
       "  0.001150565344788548,\n",
       "  0.0014799160552987209,\n",
       "  0.001247939607128501,\n",
       "  0.001145575255698835,\n",
       "  0.0010527178422004605,\n",
       "  0.0011796634757451506,\n",
       "  0.0016382176998174852,\n",
       "  0.002298732627079719,\n",
       "  0.001946812789214568,\n",
       "  0.0012910712355126937,\n",
       "  0.0013889959062604853,\n",
       "  0.0015162643847159213,\n",
       "  0.000995540891178987,\n",
       "  0.0009702936542453245,\n",
       "  0.0010126408962403527,\n",
       "  0.0011557838171332455,\n",
       "  0.0015426872356329112,\n",
       "  0.001344716257881373,\n",
       "  0.0013947421497303165,\n",
       "  0.0014725961758004916,\n",
       "  0.001193320078891702,\n",
       "  0.0011289236064638115,\n",
       "  0.0009279978099382586,\n",
       "  0.0008736134894813101,\n",
       "  0.001160608917254851,\n",
       "  0.0011720887081335403,\n",
       "  0.0011104694939503032,\n",
       "  0.0014126347655999578,\n",
       "  0.0010598337520301962,\n",
       "  0.0013263578339117682,\n",
       "  0.0017348839741316625,\n",
       "  0.0017407042640520053,\n",
       "  0.0014866336106529666,\n",
       "  0.0015429789539969836,\n",
       "  0.0019507355263663662,\n",
       "  0.0013271195561780284,\n",
       "  0.0010615654863714654,\n",
       "  0.0011105756614900504,\n",
       "  0.0010596377332048076,\n",
       "  0.0010302863961745364,\n",
       "  0.0013520844322758624,\n",
       "  0.0012322370910422048,\n",
       "  0.0010265874540588509,\n",
       "  0.0011031545847395641,\n",
       "  0.001159790806317081,\n",
       "  0.001120344323377746,\n",
       "  0.0012546569742779765,\n",
       "  0.0012127800734661933,\n",
       "  0.0010039978945213888,\n",
       "  0.000992232000054274,\n",
       "  0.0010826127231767814,\n",
       "  0.0011914892581343236,\n",
       "  0.0008993209384950912,\n",
       "  0.0008226408060484876,\n",
       "  0.00089641171378187,\n",
       "  0.0008954379254848593,\n",
       "  0.0012459869199018512,\n",
       "  0.001112035455945362,\n",
       "  0.0009811780606267147,\n",
       "  0.0010080897168437432,\n",
       "  0.0015173826327857872,\n",
       "  0.0025375549610342,\n",
       "  0.002220809155066187,\n",
       "  0.0012904796034692682,\n",
       "  0.0013304907844738207,\n",
       "  0.001434474938368011,\n",
       "  0.001388254828634672,\n",
       "  0.0016801513569791697,\n",
       "  0.001454056200842994,\n",
       "  0.0012116035883082076,\n",
       "  0.0013473675427829019,\n",
       "  0.0010919012766357305,\n",
       "  0.001582820569941153,\n",
       "  0.0010595590613471965,\n",
       "  0.0009551879991906592,\n",
       "  0.0008192278160196212,\n",
       "  0.0007989324162028627,\n",
       "  0.0011592081376066846,\n",
       "  0.001035765126774398,\n",
       "  0.001004824537732121,\n",
       "  0.00084877825526443,\n",
       "  0.0016627308860835102,\n",
       "  0.0014420569903450086,\n",
       "  0.0013032446570125306,\n",
       "  0.0019425899537357813,\n",
       "  0.0019404703667128666,\n",
       "  0.0017267290024190313,\n",
       "  0.0014141979090507245,\n",
       "  0.0013547027512686328,\n",
       "  0.001491625543632027,\n",
       "  0.001669755668586327,\n",
       "  0.001230436808610749,\n",
       "  0.0012685698023738546,\n",
       "  0.0017008037102641538,\n",
       "  0.0015291563529495357,\n",
       "  0.0013659587420988828,\n",
       "  0.0011661595871878994,\n",
       "  0.0010043528073260355,\n",
       "  0.0013224108442146745,\n",
       "  0.0011679924539445589],\n",
       " 'mae': [0.19042213,\n",
       "  0.13747905,\n",
       "  0.11797766,\n",
       "  0.109466195,\n",
       "  0.10129971,\n",
       "  0.09447888,\n",
       "  0.091113985,\n",
       "  0.08674846,\n",
       "  0.080832645,\n",
       "  0.0776258,\n",
       "  0.07466948,\n",
       "  0.07288269,\n",
       "  0.070042394,\n",
       "  0.06893169,\n",
       "  0.06541371,\n",
       "  0.064328544,\n",
       "  0.061876208,\n",
       "  0.06034138,\n",
       "  0.06052461,\n",
       "  0.057400003,\n",
       "  0.055493217,\n",
       "  0.05769348,\n",
       "  0.05377282,\n",
       "  0.052802257,\n",
       "  0.052557748,\n",
       "  0.05239649,\n",
       "  0.05175168,\n",
       "  0.051385537,\n",
       "  0.049691956,\n",
       "  0.048816446,\n",
       "  0.048782226,\n",
       "  0.046563696,\n",
       "  0.04494812,\n",
       "  0.04523909,\n",
       "  0.044819407,\n",
       "  0.04409441,\n",
       "  0.043248568,\n",
       "  0.04375666,\n",
       "  0.04680062,\n",
       "  0.04429212,\n",
       "  0.04299724,\n",
       "  0.043346215,\n",
       "  0.04352525,\n",
       "  0.041004006,\n",
       "  0.040842503,\n",
       "  0.043177556,\n",
       "  0.041787025,\n",
       "  0.040496312,\n",
       "  0.039105,\n",
       "  0.038797792,\n",
       "  0.038328882,\n",
       "  0.039610688,\n",
       "  0.03889606,\n",
       "  0.039055414,\n",
       "  0.0420077,\n",
       "  0.038359262,\n",
       "  0.03809774,\n",
       "  0.03890403,\n",
       "  0.03808144,\n",
       "  0.035762496,\n",
       "  0.03468136,\n",
       "  0.034160204,\n",
       "  0.034188956,\n",
       "  0.033223625,\n",
       "  0.03331586,\n",
       "  0.03417779,\n",
       "  0.034771863,\n",
       "  0.032683216,\n",
       "  0.030656658,\n",
       "  0.031566806,\n",
       "  0.031375315,\n",
       "  0.03286458,\n",
       "  0.033057276,\n",
       "  0.03102154,\n",
       "  0.032103155,\n",
       "  0.036456697,\n",
       "  0.038516983,\n",
       "  0.0354871,\n",
       "  0.030967427,\n",
       "  0.030995661,\n",
       "  0.032133956,\n",
       "  0.030273544,\n",
       "  0.031544548,\n",
       "  0.029844163,\n",
       "  0.032082524,\n",
       "  0.030673573,\n",
       "  0.030025003,\n",
       "  0.029931251,\n",
       "  0.037199598,\n",
       "  0.029909667,\n",
       "  0.028743505,\n",
       "  0.028468436,\n",
       "  0.027386121,\n",
       "  0.034097977,\n",
       "  0.030607289,\n",
       "  0.03073424,\n",
       "  0.028253045,\n",
       "  0.027179876,\n",
       "  0.030115563,\n",
       "  0.030248106,\n",
       "  0.029192366,\n",
       "  0.026903829,\n",
       "  0.029320352,\n",
       "  0.028663082,\n",
       "  0.028507162,\n",
       "  0.026779216,\n",
       "  0.026437039,\n",
       "  0.02546321,\n",
       "  0.023390945,\n",
       "  0.028980762,\n",
       "  0.02765803,\n",
       "  0.025553206,\n",
       "  0.027569713,\n",
       "  0.028374154,\n",
       "  0.028460857,\n",
       "  0.027189936,\n",
       "  0.025137883,\n",
       "  0.02557793,\n",
       "  0.026925644,\n",
       "  0.026622104,\n",
       "  0.029476805,\n",
       "  0.028882366,\n",
       "  0.026161615,\n",
       "  0.025672799,\n",
       "  0.024747498,\n",
       "  0.02611951,\n",
       "  0.026705327,\n",
       "  0.02810389,\n",
       "  0.026757708,\n",
       "  0.025175521,\n",
       "  0.023752417,\n",
       "  0.023744032,\n",
       "  0.02193805,\n",
       "  0.023709867,\n",
       "  0.022308748,\n",
       "  0.021817226,\n",
       "  0.021540506,\n",
       "  0.022758156,\n",
       "  0.024818698,\n",
       "  0.022085644,\n",
       "  0.02022344,\n",
       "  0.02580507,\n",
       "  0.024919113,\n",
       "  0.02357609,\n",
       "  0.02211974,\n",
       "  0.023345795,\n",
       "  0.021407131,\n",
       "  0.023192909,\n",
       "  0.02363431,\n",
       "  0.023165023,\n",
       "  0.023799354,\n",
       "  0.023917511,\n",
       "  0.02274055,\n",
       "  0.02411362,\n",
       "  0.022528691,\n",
       "  0.023039425,\n",
       "  0.023305377,\n",
       "  0.021587195,\n",
       "  0.022329954,\n",
       "  0.020887017,\n",
       "  0.022411028,\n",
       "  0.019867163,\n",
       "  0.02005343,\n",
       "  0.020394044,\n",
       "  0.021795345,\n",
       "  0.020742048,\n",
       "  0.021299459,\n",
       "  0.022001443,\n",
       "  0.020853091,\n",
       "  0.02063623,\n",
       "  0.020537544,\n",
       "  0.020278346,\n",
       "  0.020293085,\n",
       "  0.021232778,\n",
       "  0.020808335,\n",
       "  0.02369844,\n",
       "  0.022786025,\n",
       "  0.019857077,\n",
       "  0.01998164,\n",
       "  0.019563416,\n",
       "  0.020409167,\n",
       "  0.02231131,\n",
       "  0.018800633,\n",
       "  0.017748913,\n",
       "  0.018501936,\n",
       "  0.019658068,\n",
       "  0.024036756,\n",
       "  0.02233981,\n",
       "  0.02045558,\n",
       "  0.020989962,\n",
       "  0.021670569,\n",
       "  0.019923698,\n",
       "  0.019653868,\n",
       "  0.018681308,\n",
       "  0.01991333,\n",
       "  0.01988862,\n",
       "  0.020733355,\n",
       "  0.02161667,\n",
       "  0.021336857,\n",
       "  0.01927399,\n",
       "  0.019187866,\n",
       "  0.018667564,\n",
       "  0.019655062,\n",
       "  0.0227448,\n",
       "  0.020099368,\n",
       "  0.017854597,\n",
       "  0.01768075,\n",
       "  0.01777028,\n",
       "  0.018174045,\n",
       "  0.01727019,\n",
       "  0.01778656,\n",
       "  0.016976405,\n",
       "  0.017067762,\n",
       "  0.016427012,\n",
       "  0.017531157,\n",
       "  0.018598402,\n",
       "  0.018971676,\n",
       "  0.019560378,\n",
       "  0.02005079,\n",
       "  0.01922873,\n",
       "  0.018274192,\n",
       "  0.01784469,\n",
       "  0.016882883,\n",
       "  0.016901921,\n",
       "  0.017749825,\n",
       "  0.019204138,\n",
       "  0.017695792,\n",
       "  0.018498603,\n",
       "  0.02122257,\n",
       "  0.021914937,\n",
       "  0.019601092,\n",
       "  0.020806404,\n",
       "  0.01813883,\n",
       "  0.017297707,\n",
       "  0.016950812,\n",
       "  0.016770992,\n",
       "  0.017354732,\n",
       "  0.015792524,\n",
       "  0.015444583,\n",
       "  0.014436557,\n",
       "  0.014961277,\n",
       "  0.016635686,\n",
       "  0.01698104,\n",
       "  0.01760633,\n",
       "  0.021856874,\n",
       "  0.019170268,\n",
       "  0.019909594,\n",
       "  0.018510798,\n",
       "  0.016773676,\n",
       "  0.017628402,\n",
       "  0.017182108,\n",
       "  0.01648023,\n",
       "  0.016405305,\n",
       "  0.019296449,\n",
       "  0.017861938,\n",
       "  0.016956009,\n",
       "  0.015299841,\n",
       "  0.015597474,\n",
       "  0.015488263,\n",
       "  0.01645756,\n",
       "  0.017184822,\n",
       "  0.015666593,\n",
       "  0.016959341,\n",
       "  0.01774495,\n",
       "  0.01898935,\n",
       "  0.017664798,\n",
       "  0.017413117,\n",
       "  0.016991004,\n",
       "  0.016402638,\n",
       "  0.016501164,\n",
       "  0.01469894,\n",
       "  0.013914962,\n",
       "  0.015379026,\n",
       "  0.016517652,\n",
       "  0.0162347,\n",
       "  0.015054638,\n",
       "  0.014905594,\n",
       "  0.019460315,\n",
       "  0.017107965,\n",
       "  0.016230961,\n",
       "  0.01624545,\n",
       "  0.015913665,\n",
       "  0.016109126,\n",
       "  0.015216553,\n",
       "  0.01740489,\n",
       "  0.016385911,\n",
       "  0.017091272,\n",
       "  0.016211042,\n",
       "  0.01620706,\n",
       "  0.019389154,\n",
       "  0.016892442,\n",
       "  0.015655333,\n",
       "  0.016531488,\n",
       "  0.016888648,\n",
       "  0.016514067,\n",
       "  0.015931197,\n",
       "  0.016447073,\n",
       "  0.015807247,\n",
       "  0.014066732,\n",
       "  0.014132405,\n",
       "  0.015065774,\n",
       "  0.0153945815,\n",
       "  0.013767851,\n",
       "  0.013095104,\n",
       "  0.013621251,\n",
       "  0.015429191,\n",
       "  0.013827498,\n",
       "  0.014820736,\n",
       "  0.017273327,\n",
       "  0.01622284,\n",
       "  0.015277557,\n",
       "  0.017615145,\n",
       "  0.015974572,\n",
       "  0.019424098,\n",
       "  0.022364326,\n",
       "  0.01753779,\n",
       "  0.016353987,\n",
       "  0.02050884,\n",
       "  0.017967973,\n",
       "  0.015186702,\n",
       "  0.014539538,\n",
       "  0.014686751,\n",
       "  0.015605594,\n",
       "  0.015429168,\n",
       "  0.014072979,\n",
       "  0.013912182,\n",
       "  0.01323875,\n",
       "  0.013321876,\n",
       "  0.014485475,\n",
       "  0.015101927,\n",
       "  0.015648495,\n",
       "  0.014823841,\n",
       "  0.016730608,\n",
       "  0.014802147,\n",
       "  0.014651853,\n",
       "  0.013172573,\n",
       "  0.0136834495,\n",
       "  0.0158597,\n",
       "  0.015108412,\n",
       "  0.015881205,\n",
       "  0.015738228,\n",
       "  0.01384761,\n",
       "  0.015713204,\n",
       "  0.016014084,\n",
       "  0.016231474,\n",
       "  0.017903168,\n",
       "  0.01596334,\n",
       "  0.016961452,\n",
       "  0.015631484,\n",
       "  0.015896203,\n",
       "  0.018801538,\n",
       "  0.017355768,\n",
       "  0.015694361,\n",
       "  0.015777376,\n",
       "  0.0155457975,\n",
       "  0.013766441,\n",
       "  0.015765991,\n",
       "  0.015219676,\n",
       "  0.014282192,\n",
       "  0.013009447,\n",
       "  0.0140749505,\n",
       "  0.018821089,\n",
       "  0.016656574,\n",
       "  0.016099975,\n",
       "  0.016285656,\n",
       "  0.02007879,\n",
       "  0.020584224,\n",
       "  0.016937645,\n",
       "  0.015997138,\n",
       "  0.015737263,\n",
       "  0.017259827,\n",
       "  0.018662285,\n",
       "  0.01747384,\n",
       "  0.017749816,\n",
       "  0.017462013,\n",
       "  0.0150098335,\n",
       "  0.015066322,\n",
       "  0.01423798,\n",
       "  0.014193062,\n",
       "  0.0149883665,\n",
       "  0.013666733,\n",
       "  0.013615694,\n",
       "  0.015592185,\n",
       "  0.016091226,\n",
       "  0.015193451,\n",
       "  0.013448959,\n",
       "  0.016365731,\n",
       "  0.01774223,\n",
       "  0.014636537,\n",
       "  0.013161809,\n",
       "  0.01584039,\n",
       "  0.017620498,\n",
       "  0.018717512,\n",
       "  0.015746737,\n",
       "  0.014506593,\n",
       "  0.01501879,\n",
       "  0.013257091,\n",
       "  0.01437375,\n",
       "  0.014719684,\n",
       "  0.015455364,\n",
       "  0.014788838,\n",
       "  0.015268514,\n",
       "  0.015186866,\n",
       "  0.013596667,\n",
       "  0.013776681,\n",
       "  0.014688221,\n",
       "  0.016097145,\n",
       "  0.020303713,\n",
       "  0.018537765,\n",
       "  0.015362754,\n",
       "  0.015434496,\n",
       "  0.016308796,\n",
       "  0.013942008,\n",
       "  0.012622144,\n",
       "  0.012807607,\n",
       "  0.013655252,\n",
       "  0.016246734,\n",
       "  0.015005098,\n",
       "  0.0153138265,\n",
       "  0.016216753,\n",
       "  0.01501067,\n",
       "  0.013661531,\n",
       "  0.0128649855,\n",
       "  0.012265984,\n",
       "  0.013301377,\n",
       "  0.014492463,\n",
       "  0.013539818,\n",
       "  0.014387578,\n",
       "  0.012921516,\n",
       "  0.014934884,\n",
       "  0.015882455,\n",
       "  0.016959254,\n",
       "  0.016035898,\n",
       "  0.015145749,\n",
       "  0.01748511,\n",
       "  0.015123801,\n",
       "  0.013155696,\n",
       "  0.012584392,\n",
       "  0.0126793645,\n",
       "  0.013743031,\n",
       "  0.015143622,\n",
       "  0.014784245,\n",
       "  0.013042693,\n",
       "  0.01301261,\n",
       "  0.013838644,\n",
       "  0.012582326,\n",
       "  0.014597932,\n",
       "  0.014168452,\n",
       "  0.012701755,\n",
       "  0.012340804,\n",
       "  0.012390362,\n",
       "  0.013446848,\n",
       "  0.012772031,\n",
       "  0.0111236265,\n",
       "  0.012064441,\n",
       "  0.011707055,\n",
       "  0.013092642,\n",
       "  0.012971415,\n",
       "  0.012928459,\n",
       "  0.01200893,\n",
       "  0.015025379,\n",
       "  0.019377569,\n",
       "  0.01946475,\n",
       "  0.014380535,\n",
       "  0.015136443,\n",
       "  0.015316429,\n",
       "  0.015260603,\n",
       "  0.016942788,\n",
       "  0.015768662,\n",
       "  0.014431258,\n",
       "  0.014197582,\n",
       "  0.014224527,\n",
       "  0.015436369,\n",
       "  0.012978578,\n",
       "  0.012414558,\n",
       "  0.01132073,\n",
       "  0.011653478,\n",
       "  0.013665058,\n",
       "  0.012803641,\n",
       "  0.012692014,\n",
       "  0.011876677,\n",
       "  0.015877057,\n",
       "  0.014662306,\n",
       "  0.014673807,\n",
       "  0.016558198,\n",
       "  0.019045189,\n",
       "  0.0166522,\n",
       "  0.016058035,\n",
       "  0.01505347,\n",
       "  0.01614967,\n",
       "  0.016090684,\n",
       "  0.014372094,\n",
       "  0.014171666,\n",
       "  0.01607937,\n",
       "  0.01528996,\n",
       "  0.015015565,\n",
       "  0.014685273,\n",
       "  0.01322178,\n",
       "  0.015524466,\n",
       "  0.013403617],\n",
       " 'val_loss': [0.03368329796940088,\n",
       "  0.025369911268353463,\n",
       "  0.01969678746536374,\n",
       "  0.020921416580677032,\n",
       "  0.018543710839003323,\n",
       "  0.017515506688505413,\n",
       "  0.01676873993128538,\n",
       "  0.016331714671105148,\n",
       "  0.01480173533782363,\n",
       "  0.015008165035396814,\n",
       "  0.014393784943968058,\n",
       "  0.015379560738801956,\n",
       "  0.01417743032798171,\n",
       "  0.013785732723772526,\n",
       "  0.01310696005821228,\n",
       "  0.013898890558630228,\n",
       "  0.01353062279522419,\n",
       "  0.012897253409028054,\n",
       "  0.013681228458881377,\n",
       "  0.01252985717728734,\n",
       "  0.01333239427767694,\n",
       "  0.013527870178222656,\n",
       "  0.012249995907768606,\n",
       "  0.012662091292440891,\n",
       "  0.01253186035901308,\n",
       "  0.013569359760731458,\n",
       "  0.012587164156138897,\n",
       "  0.011800839006900788,\n",
       "  0.012777686771005391,\n",
       "  0.012825739942491055,\n",
       "  0.012192602455615997,\n",
       "  0.011521773971617221,\n",
       "  0.012165952986106277,\n",
       "  0.012309440644457936,\n",
       "  0.012518240325152873,\n",
       "  0.011426832946017384,\n",
       "  0.011313686240464449,\n",
       "  0.013345835451036691,\n",
       "  0.013950373325496913,\n",
       "  0.011840919498354197,\n",
       "  0.011562765901908278,\n",
       "  0.01287986133247614,\n",
       "  0.011658591963350774,\n",
       "  0.013004369474947453,\n",
       "  0.01153312218375504,\n",
       "  0.013614656776189804,\n",
       "  0.011840341752395034,\n",
       "  0.012340920977294445,\n",
       "  0.011292999796569347,\n",
       "  0.012447798391804099,\n",
       "  0.012706173909828066,\n",
       "  0.012118860986083746,\n",
       "  0.011388805322349071,\n",
       "  0.012957369349896908,\n",
       "  0.011695902748033404,\n",
       "  0.012190536921843886,\n",
       "  0.011213438119739294,\n",
       "  0.011593117285519839,\n",
       "  0.010542959161102771,\n",
       "  0.010774614755064249,\n",
       "  0.010419602319598199,\n",
       "  0.01142624719068408,\n",
       "  0.012077610474079847,\n",
       "  0.010677351942285895,\n",
       "  0.010762548260390758,\n",
       "  0.010751211130991579,\n",
       "  0.011414444632828235,\n",
       "  0.010955477738752962,\n",
       "  0.010239729192107916,\n",
       "  0.010631645331159234,\n",
       "  0.011531739495694637,\n",
       "  0.011075381003320218,\n",
       "  0.011912937182933092,\n",
       "  0.010866450145840645,\n",
       "  0.012281616823747754,\n",
       "  0.014290209021419286,\n",
       "  0.012056306470185518,\n",
       "  0.01158405775204301,\n",
       "  0.010685882624238729,\n",
       "  0.01076386566273868,\n",
       "  0.011914782598614693,\n",
       "  0.011235071439296007,\n",
       "  0.010839243791997433,\n",
       "  0.010665921494364738,\n",
       "  0.01102942950092256,\n",
       "  0.0100246321875602,\n",
       "  0.010472210589796305,\n",
       "  0.011005699913948775,\n",
       "  0.010619181022047997,\n",
       "  0.009677227959036827,\n",
       "  0.011149128945544363,\n",
       "  0.010318068414926529,\n",
       "  0.012296408880501986,\n",
       "  0.01168559892103076,\n",
       "  0.011134732142090797,\n",
       "  0.010747155779972673,\n",
       "  0.011236970080062747,\n",
       "  0.01185535783879459,\n",
       "  0.011058313120156527,\n",
       "  0.01149140321649611,\n",
       "  0.010194834973663091,\n",
       "  0.01167364134453237,\n",
       "  0.011996827321127057,\n",
       "  0.01135120908729732,\n",
       "  0.011244871141389012,\n",
       "  0.010452410206198692,\n",
       "  0.011001459322869778,\n",
       "  0.011185287125408649,\n",
       "  0.010474524414166807,\n",
       "  0.011728265043348073,\n",
       "  0.011237383214756846,\n",
       "  0.011792559828609227,\n",
       "  0.011025632917881011,\n",
       "  0.010703766345977783,\n",
       "  0.011203125258907676,\n",
       "  0.010370672354474664,\n",
       "  0.009877867065370083,\n",
       "  0.011153804138302803,\n",
       "  0.010863389773294329,\n",
       "  0.011376049695536494,\n",
       "  0.012506151851266623,\n",
       "  0.011352666141465306,\n",
       "  0.010814438946545124,\n",
       "  0.010016619041562081,\n",
       "  0.010575129091739655,\n",
       "  0.01218432392925024,\n",
       "  0.010969976149499416,\n",
       "  0.01142867999151349,\n",
       "  0.010299524944275617,\n",
       "  0.010802289610728622,\n",
       "  0.010963443620130419,\n",
       "  0.010503626987338066,\n",
       "  0.010380464093759655,\n",
       "  0.011312328046187758,\n",
       "  0.010979661252349615,\n",
       "  0.01043035238981247,\n",
       "  0.010236117849126458,\n",
       "  0.012348794378340244,\n",
       "  0.010130558814853429,\n",
       "  0.009472529450431467,\n",
       "  0.00978868748061359,\n",
       "  0.011125955451279878,\n",
       "  0.010480026621371508,\n",
       "  0.010561348032206297,\n",
       "  0.010986389080062508,\n",
       "  0.010007623070850968,\n",
       "  0.011008239164948464,\n",
       "  0.010376814939081668,\n",
       "  0.010379974823445081,\n",
       "  0.010136911878362298,\n",
       "  0.009424505522474647,\n",
       "  0.010091583849862218,\n",
       "  0.00961946495808661,\n",
       "  0.010560884047299624,\n",
       "  0.010664761159569025,\n",
       "  0.01046102805994451,\n",
       "  0.010304709244519473,\n",
       "  0.010751320980489254,\n",
       "  0.009689142415300011,\n",
       "  0.00971670993603766,\n",
       "  0.010216165147721768,\n",
       "  0.009508340898901225,\n",
       "  0.009793071402236819,\n",
       "  0.009858328476548195,\n",
       "  0.010862710513174533,\n",
       "  0.010911083314567804,\n",
       "  0.010879179136827588,\n",
       "  0.010303192678838969,\n",
       "  0.010470704408362508,\n",
       "  0.009867654368281364,\n",
       "  0.009689397504553198,\n",
       "  0.009575784718617798,\n",
       "  0.010307590244337916,\n",
       "  0.010779932048171759,\n",
       "  0.011066707922145725,\n",
       "  0.01051400494761765,\n",
       "  0.010362523375079036,\n",
       "  0.010246074572205544,\n",
       "  0.011256651300936938,\n",
       "  0.009733399329707026,\n",
       "  0.010977504262700676,\n",
       "  0.011026789247989655,\n",
       "  0.010057789646089077,\n",
       "  0.010005302121862769,\n",
       "  0.01001805798150599,\n",
       "  0.011263577360659837,\n",
       "  0.010626107128337026,\n",
       "  0.009843569761142135,\n",
       "  0.010409031715244054,\n",
       "  0.011069716978818178,\n",
       "  0.010778802447021008,\n",
       "  0.010371247259899973,\n",
       "  0.010402252757921815,\n",
       "  0.01018625064752996,\n",
       "  0.009762752195820212,\n",
       "  0.010814415803179146,\n",
       "  0.01007718900218606,\n",
       "  0.0128475078381598,\n",
       "  0.010565366921946406,\n",
       "  0.010295490827411413,\n",
       "  0.010410155821591615,\n",
       "  0.01047014193609357,\n",
       "  0.010752713400870561,\n",
       "  0.010176506452262401,\n",
       "  0.009653702192008496,\n",
       "  0.00980540201999247,\n",
       "  0.01075368351303041,\n",
       "  0.01025290577672422,\n",
       "  0.010318460874259472,\n",
       "  0.010574691509827972,\n",
       "  0.009948351280763745,\n",
       "  0.010162752959877253,\n",
       "  0.009854468004778027,\n",
       "  0.010121188499033451,\n",
       "  0.009730612998828292,\n",
       "  0.011102121183648705,\n",
       "  0.011154394177719951,\n",
       "  0.010478139948099851,\n",
       "  0.009677694085985422,\n",
       "  0.009318818850442768,\n",
       "  0.00948686497285962,\n",
       "  0.00958199859596789,\n",
       "  0.00864617144688964,\n",
       "  0.009457552060484885,\n",
       "  0.009633469954133033,\n",
       "  0.009707914851605891,\n",
       "  0.009529855381697417,\n",
       "  0.011043285485357046,\n",
       "  0.010227732919156551,\n",
       "  0.010054233111441135,\n",
       "  0.010282986797392368,\n",
       "  0.010136664798483252,\n",
       "  0.009194043930619955,\n",
       "  0.008615071559324861,\n",
       "  0.009606695547699929,\n",
       "  0.010600730776786804,\n",
       "  0.010373477963730693,\n",
       "  0.009565230738371611,\n",
       "  0.010437009669840337,\n",
       "  0.01006709118373692,\n",
       "  0.009234602237120271,\n",
       "  0.010090813646093011,\n",
       "  0.010262368386611343,\n",
       "  0.010412819450721145,\n",
       "  0.01075820098631084,\n",
       "  0.011654458194971084,\n",
       "  0.010449736751616001,\n",
       "  0.00920993029139936,\n",
       "  0.00941394092515111,\n",
       "  0.009604572784155607,\n",
       "  0.009366778284311294,\n",
       "  0.00942087103612721,\n",
       "  0.010051625967025756,\n",
       "  0.010223279148340226,\n",
       "  0.009644802007824183,\n",
       "  0.008676188811659814,\n",
       "  0.009075886802747846,\n",
       "  0.009047795739024877,\n",
       "  0.00932686305604875,\n",
       "  0.009646769845858216,\n",
       "  0.009892295068129896,\n",
       "  0.008782977005466819,\n",
       "  0.008865703735500574,\n",
       "  0.011361231794580817,\n",
       "  0.009636355377733708,\n",
       "  0.01014316249638796,\n",
       "  0.011112357350066304,\n",
       "  0.010298190964385867,\n",
       "  0.009539542300626636,\n",
       "  0.008937271637842059,\n",
       "  0.009050949616357684,\n",
       "  0.008890716172754765,\n",
       "  0.010312279779464006,\n",
       "  0.010470764432102441,\n",
       "  0.011053785728290678,\n",
       "  0.010539154056459665,\n",
       "  0.009962998749688267,\n",
       "  0.010975249018520117,\n",
       "  0.009327704645693302,\n",
       "  0.009862790815532207,\n",
       "  0.011100063845515252,\n",
       "  0.00957026369869709,\n",
       "  0.009804795030504465,\n",
       "  0.009702192014083266,\n",
       "  0.00987261370755732,\n",
       "  0.009438870288431645,\n",
       "  0.00962843126617372,\n",
       "  0.00953175532631576,\n",
       "  0.010508190747350455,\n",
       "  0.010121219279244541,\n",
       "  0.009487548330798744,\n",
       "  0.009714202024042606,\n",
       "  0.01014945823699236,\n",
       "  0.010315610002726317,\n",
       "  0.009790359623730182,\n",
       "  0.009895335882902145,\n",
       "  0.011001981515437364,\n",
       "  0.010591496899724006,\n",
       "  0.010262470925226808,\n",
       "  0.010140385618433357,\n",
       "  0.009736624266952276,\n",
       "  0.009871632559224963,\n",
       "  0.009197577042505145,\n",
       "  0.009191807871684433,\n",
       "  0.009620705619454384,\n",
       "  0.009845815319567918,\n",
       "  0.009491128753870726,\n",
       "  0.010073600756004452,\n",
       "  0.01038624313659966,\n",
       "  0.010412228014320134,\n",
       "  0.009277241630479693,\n",
       "  0.009834237908944488,\n",
       "  0.010558380931615829,\n",
       "  0.011963376030325889,\n",
       "  0.010172470146790147,\n",
       "  0.010263327043503523,\n",
       "  0.009633640991523862,\n",
       "  0.00994355883449316,\n",
       "  0.009856944996863603,\n",
       "  0.00949526303447783,\n",
       "  0.010292860446497798,\n",
       "  0.00955160716548562,\n",
       "  0.009728116914629936,\n",
       "  0.009413748467341065,\n",
       "  0.008938678167760372,\n",
       "  0.00890846927650273,\n",
       "  0.00864571426063776,\n",
       "  0.008587063988670707,\n",
       "  0.009146576235070824,\n",
       "  0.009518570639193058,\n",
       "  0.009230907307937741,\n",
       "  0.008873675088398158,\n",
       "  0.00966637977398932,\n",
       "  0.008996934490278363,\n",
       "  0.008789294585585593,\n",
       "  0.009479582449421287,\n",
       "  0.009287991328164935,\n",
       "  0.009570365771651268,\n",
       "  0.009964603139087558,\n",
       "  0.010427734535187482,\n",
       "  0.009753570938482881,\n",
       "  0.008887980226427316,\n",
       "  0.009780448861420155,\n",
       "  0.009021841315552592,\n",
       "  0.01002326630987227,\n",
       "  0.00964194694533944,\n",
       "  0.009267207235097885,\n",
       "  0.009558759024366736,\n",
       "  0.009697599755600094,\n",
       "  0.010946226958185434,\n",
       "  0.010322094522416591,\n",
       "  0.009342887811362744,\n",
       "  0.00898246532306075,\n",
       "  0.009039095602929592,\n",
       "  0.00894529870711267,\n",
       "  0.010238509345799684,\n",
       "  0.009716508723795415,\n",
       "  0.010020577581599355,\n",
       "  0.009112408850342035,\n",
       "  0.009018935635685921,\n",
       "  0.009636089252308012,\n",
       "  0.010298405401408672,\n",
       "  0.01103123016655445,\n",
       "  0.009602070599794389,\n",
       "  0.010708177788183093,\n",
       "  0.011480948934331537,\n",
       "  0.010363226570189,\n",
       "  0.00999988866969943,\n",
       "  0.009928487055003644,\n",
       "  0.010021507507190108,\n",
       "  0.010747091937810182,\n",
       "  0.011085231322795153,\n",
       "  0.010861544171348214,\n",
       "  0.009253971697762608,\n",
       "  0.010027617355808616,\n",
       "  0.00974746560677886,\n",
       "  0.009295682003721596,\n",
       "  0.009234006330370904,\n",
       "  0.009352545207366347,\n",
       "  0.010065649030730128,\n",
       "  0.00902921548113227,\n",
       "  0.009377415664494038,\n",
       "  0.009268616419285535,\n",
       "  0.010212772293016315,\n",
       "  0.00912181893363595,\n",
       "  0.008746995916590094,\n",
       "  0.01079919789917767,\n",
       "  0.009681969787925482,\n",
       "  0.008708488522097468,\n",
       "  0.00981074352748692,\n",
       "  0.009806223958730698,\n",
       "  0.010870899120345712,\n",
       "  0.01008951235562563,\n",
       "  0.010444681439548732,\n",
       "  0.010676653077825903,\n",
       "  0.009436739375814795,\n",
       "  0.0092401506844908,\n",
       "  0.009828514279797673,\n",
       "  0.009408270800486207,\n",
       "  0.009252458345144987,\n",
       "  0.00954112857580185,\n",
       "  0.009723144443705678,\n",
       "  0.009738212777301669,\n",
       "  0.00912694027647376,\n",
       "  0.010225971974432469,\n",
       "  0.009716318733990193,\n",
       "  0.010563442250713706,\n",
       "  0.011454829899594187,\n",
       "  0.009192792000249028,\n",
       "  0.010306926630437373,\n",
       "  0.009946367726661265,\n",
       "  0.010259640775620938,\n",
       "  0.009343709889799356,\n",
       "  0.009403375163674354,\n",
       "  0.009164060652256011,\n",
       "  0.01058810893446207,\n",
       "  0.009344392688944936,\n",
       "  0.009736917586997152,\n",
       "  0.009391023451462388,\n",
       "  0.010477905487641692,\n",
       "  0.009582843352109193,\n",
       "  0.009363732486963271,\n",
       "  0.009584817430004477,\n",
       "  0.009182710852473974,\n",
       "  0.01017748871818185,\n",
       "  0.010134482895955444,\n",
       "  0.009318587835878134,\n",
       "  0.009136091032996774,\n",
       "  0.009840247873216867,\n",
       "  0.008501595398411155,\n",
       "  0.010393000580370427,\n",
       "  0.009160104906186461,\n",
       "  0.010543273016810416,\n",
       "  0.00845646788366139,\n",
       "  0.009734382433816791,\n",
       "  0.00927399336360395,\n",
       "  0.009691595286130904,\n",
       "  0.009423108026385307,\n",
       "  0.009724033530801535,\n",
       "  0.009768352145329118,\n",
       "  0.009531228570267558,\n",
       "  0.010188414342701435,\n",
       "  0.009868469275534152,\n",
       "  0.00982547840103507,\n",
       "  0.009312887815758586,\n",
       "  0.009629073552787304,\n",
       "  0.010456420946866274,\n",
       "  0.009302914701402187,\n",
       "  0.009349776711314917,\n",
       "  0.010476807598024607,\n",
       "  0.009796800091862679,\n",
       "  0.0101762636564672,\n",
       "  0.010823426162824034,\n",
       "  0.010509379580616951,\n",
       "  0.009791243728250265,\n",
       "  0.009791389433667063,\n",
       "  0.010679152142256499,\n",
       "  0.009787920583039521,\n",
       "  0.009418461099267006,\n",
       "  0.009286390803754329,\n",
       "  0.010617873258888721,\n",
       "  0.01225615837611258,\n",
       "  0.010062719089910388,\n",
       "  0.009067697171121835,\n",
       "  0.009745767246931792,\n",
       "  0.010221679927781224,\n",
       "  0.011420074524357915,\n",
       "  0.009828779427334667,\n",
       "  0.00978556894697249,\n",
       "  0.009887410281226038,\n",
       "  0.00955506069585681,\n",
       "  0.010779064195230604,\n",
       "  0.009494588989764453,\n",
       "  0.010058829095214605,\n",
       "  0.009204695979133249,\n",
       "  0.00876735565252602,\n",
       "  0.008845678018406033,\n",
       "  0.009103627176955342,\n",
       "  0.009333721082657575,\n",
       "  0.009660562779754401,\n",
       "  0.00995166702196002,\n",
       "  0.00976690985262394,\n",
       "  0.009415741125121713,\n",
       "  0.009695857483893632,\n",
       "  0.010836409963667392,\n",
       "  0.010310269659385085,\n",
       "  0.010507003124803304,\n",
       "  0.009613340999931098,\n",
       "  0.010168859083205462,\n",
       "  0.009979951055720448,\n",
       "  0.009583947900682687,\n",
       "  0.010828898241743445,\n",
       "  0.011955146305263042,\n",
       "  0.009464365709573031,\n",
       "  0.009512723004445434,\n",
       "  0.009025681391358375,\n",
       "  0.009303881041705608,\n",
       "  0.009437846951186657,\n",
       "  0.00922302044928074,\n",
       "  0.009499573893845082],\n",
       " 'val_mae': [0.14509073,\n",
       "  0.12413857,\n",
       "  0.100630514,\n",
       "  0.1085636,\n",
       "  0.09782099,\n",
       "  0.090736546,\n",
       "  0.09241049,\n",
       "  0.08691938,\n",
       "  0.07723007,\n",
       "  0.07739942,\n",
       "  0.07771274,\n",
       "  0.08055651,\n",
       "  0.07175077,\n",
       "  0.071478404,\n",
       "  0.06821776,\n",
       "  0.0713228,\n",
       "  0.070546135,\n",
       "  0.066855416,\n",
       "  0.070394844,\n",
       "  0.064071305,\n",
       "  0.06799885,\n",
       "  0.06693673,\n",
       "  0.061847158,\n",
       "  0.06085817,\n",
       "  0.062529236,\n",
       "  0.063284166,\n",
       "  0.06335131,\n",
       "  0.06017497,\n",
       "  0.06131555,\n",
       "  0.061808266,\n",
       "  0.06159027,\n",
       "  0.05559621,\n",
       "  0.05820441,\n",
       "  0.055948503,\n",
       "  0.061576135,\n",
       "  0.05546862,\n",
       "  0.05812714,\n",
       "  0.05797531,\n",
       "  0.06387604,\n",
       "  0.05597248,\n",
       "  0.056189675,\n",
       "  0.057764918,\n",
       "  0.054510262,\n",
       "  0.05756224,\n",
       "  0.055356435,\n",
       "  0.06634542,\n",
       "  0.056727752,\n",
       "  0.05605581,\n",
       "  0.05377426,\n",
       "  0.053801555,\n",
       "  0.05303844,\n",
       "  0.05399209,\n",
       "  0.053637628,\n",
       "  0.060185146,\n",
       "  0.05434691,\n",
       "  0.05375604,\n",
       "  0.05161044,\n",
       "  0.05481499,\n",
       "  0.049885727,\n",
       "  0.050692048,\n",
       "  0.048404884,\n",
       "  0.0569458,\n",
       "  0.053269073,\n",
       "  0.049332228,\n",
       "  0.052398358,\n",
       "  0.0496622,\n",
       "  0.05162587,\n",
       "  0.0490225,\n",
       "  0.04693101,\n",
       "  0.047693245,\n",
       "  0.052316256,\n",
       "  0.050827637,\n",
       "  0.056515284,\n",
       "  0.047934107,\n",
       "  0.049532127,\n",
       "  0.06069697,\n",
       "  0.054202247,\n",
       "  0.050155226,\n",
       "  0.049175747,\n",
       "  0.04859263,\n",
       "  0.050796054,\n",
       "  0.052586015,\n",
       "  0.050980434,\n",
       "  0.047631945,\n",
       "  0.049507674,\n",
       "  0.045969717,\n",
       "  0.047162347,\n",
       "  0.049304266,\n",
       "  0.04968012,\n",
       "  0.04551117,\n",
       "  0.0487343,\n",
       "  0.045386977,\n",
       "  0.048642386,\n",
       "  0.052201804,\n",
       "  0.04651948,\n",
       "  0.046665404,\n",
       "  0.04779297,\n",
       "  0.049451467,\n",
       "  0.048108853,\n",
       "  0.047533512,\n",
       "  0.046594467,\n",
       "  0.0496544,\n",
       "  0.05128047,\n",
       "  0.048768498,\n",
       "  0.047005355,\n",
       "  0.04739276,\n",
       "  0.0459648,\n",
       "  0.046058737,\n",
       "  0.04299056,\n",
       "  0.047909837,\n",
       "  0.046473093,\n",
       "  0.04859622,\n",
       "  0.047131326,\n",
       "  0.04611656,\n",
       "  0.04754591,\n",
       "  0.047849495,\n",
       "  0.04282564,\n",
       "  0.051086515,\n",
       "  0.045846544,\n",
       "  0.04717095,\n",
       "  0.051305227,\n",
       "  0.046736326,\n",
       "  0.044645853,\n",
       "  0.043049183,\n",
       "  0.045690916,\n",
       "  0.05050924,\n",
       "  0.04717601,\n",
       "  0.047567435,\n",
       "  0.047529925,\n",
       "  0.046080727,\n",
       "  0.046277512,\n",
       "  0.04363766,\n",
       "  0.043167,\n",
       "  0.045161344,\n",
       "  0.045504343,\n",
       "  0.04338063,\n",
       "  0.042077687,\n",
       "  0.05174798,\n",
       "  0.042449795,\n",
       "  0.04050273,\n",
       "  0.042315792,\n",
       "  0.045276172,\n",
       "  0.044526562,\n",
       "  0.043032896,\n",
       "  0.04439049,\n",
       "  0.043542273,\n",
       "  0.044390168,\n",
       "  0.042991903,\n",
       "  0.04585455,\n",
       "  0.040950913,\n",
       "  0.0411219,\n",
       "  0.04551954,\n",
       "  0.040547073,\n",
       "  0.043904815,\n",
       "  0.04457193,\n",
       "  0.042381343,\n",
       "  0.045244157,\n",
       "  0.044585794,\n",
       "  0.04304494,\n",
       "  0.03899069,\n",
       "  0.04251001,\n",
       "  0.03935998,\n",
       "  0.040182736,\n",
       "  0.04130449,\n",
       "  0.042931646,\n",
       "  0.044539634,\n",
       "  0.04745924,\n",
       "  0.041314423,\n",
       "  0.041879073,\n",
       "  0.043790817,\n",
       "  0.039988026,\n",
       "  0.040291995,\n",
       "  0.042604595,\n",
       "  0.04241223,\n",
       "  0.043466676,\n",
       "  0.044583853,\n",
       "  0.042372953,\n",
       "  0.041578066,\n",
       "  0.046101365,\n",
       "  0.039804045,\n",
       "  0.045009486,\n",
       "  0.04212623,\n",
       "  0.04060527,\n",
       "  0.039688803,\n",
       "  0.040326163,\n",
       "  0.043194402,\n",
       "  0.043224502,\n",
       "  0.041236565,\n",
       "  0.041096754,\n",
       "  0.044121116,\n",
       "  0.045470554,\n",
       "  0.042579394,\n",
       "  0.041190006,\n",
       "  0.04166765,\n",
       "  0.040910337,\n",
       "  0.042291008,\n",
       "  0.039796006,\n",
       "  0.048579734,\n",
       "  0.044756263,\n",
       "  0.042877946,\n",
       "  0.041473683,\n",
       "  0.04171789,\n",
       "  0.04274003,\n",
       "  0.04317404,\n",
       "  0.042166527,\n",
       "  0.04154167,\n",
       "  0.04234442,\n",
       "  0.040818017,\n",
       "  0.039104007,\n",
       "  0.041715004,\n",
       "  0.04023497,\n",
       "  0.040131796,\n",
       "  0.04006933,\n",
       "  0.03895199,\n",
       "  0.039258804,\n",
       "  0.04120975,\n",
       "  0.04287103,\n",
       "  0.042195108,\n",
       "  0.039976843,\n",
       "  0.04058948,\n",
       "  0.03856811,\n",
       "  0.039266266,\n",
       "  0.03677802,\n",
       "  0.03734408,\n",
       "  0.03838386,\n",
       "  0.039389864,\n",
       "  0.037795585,\n",
       "  0.043891847,\n",
       "  0.04398459,\n",
       "  0.041547395,\n",
       "  0.043615237,\n",
       "  0.041103274,\n",
       "  0.03638639,\n",
       "  0.03666965,\n",
       "  0.03808621,\n",
       "  0.040525604,\n",
       "  0.039609015,\n",
       "  0.03913774,\n",
       "  0.039090984,\n",
       "  0.038511638,\n",
       "  0.03645304,\n",
       "  0.03903533,\n",
       "  0.040267486,\n",
       "  0.04245758,\n",
       "  0.041551143,\n",
       "  0.04234306,\n",
       "  0.040005848,\n",
       "  0.038292278,\n",
       "  0.038882464,\n",
       "  0.03785075,\n",
       "  0.038366713,\n",
       "  0.039407045,\n",
       "  0.040057573,\n",
       "  0.043742202,\n",
       "  0.039850734,\n",
       "  0.035426777,\n",
       "  0.038195048,\n",
       "  0.036007125,\n",
       "  0.03782226,\n",
       "  0.038432818,\n",
       "  0.038985584,\n",
       "  0.0367877,\n",
       "  0.037246104,\n",
       "  0.04358387,\n",
       "  0.037642047,\n",
       "  0.040915504,\n",
       "  0.043049544,\n",
       "  0.039976325,\n",
       "  0.037869174,\n",
       "  0.036998004,\n",
       "  0.036100443,\n",
       "  0.03574248,\n",
       "  0.040313322,\n",
       "  0.040782552,\n",
       "  0.04031623,\n",
       "  0.039563656,\n",
       "  0.03932714,\n",
       "  0.046440158,\n",
       "  0.037691675,\n",
       "  0.038258687,\n",
       "  0.04286865,\n",
       "  0.037011318,\n",
       "  0.037703834,\n",
       "  0.039064508,\n",
       "  0.038603447,\n",
       "  0.0379481,\n",
       "  0.039284196,\n",
       "  0.037642863,\n",
       "  0.046568595,\n",
       "  0.04085014,\n",
       "  0.039289184,\n",
       "  0.038677603,\n",
       "  0.043725614,\n",
       "  0.039691262,\n",
       "  0.03823278,\n",
       "  0.039781135,\n",
       "  0.03954481,\n",
       "  0.0392605,\n",
       "  0.038466662,\n",
       "  0.04020028,\n",
       "  0.038919464,\n",
       "  0.0372737,\n",
       "  0.035538092,\n",
       "  0.034596972,\n",
       "  0.036480162,\n",
       "  0.03699095,\n",
       "  0.03655511,\n",
       "  0.038852308,\n",
       "  0.039066564,\n",
       "  0.038869224,\n",
       "  0.0375511,\n",
       "  0.037842985,\n",
       "  0.039857656,\n",
       "  0.046016138,\n",
       "  0.042876452,\n",
       "  0.04147462,\n",
       "  0.03873256,\n",
       "  0.03966447,\n",
       "  0.041112028,\n",
       "  0.040449396,\n",
       "  0.040177148,\n",
       "  0.03754612,\n",
       "  0.039296884,\n",
       "  0.037839334,\n",
       "  0.037817538,\n",
       "  0.036556084,\n",
       "  0.034807473,\n",
       "  0.035769403,\n",
       "  0.038227275,\n",
       "  0.038105845,\n",
       "  0.037011433,\n",
       "  0.037757475,\n",
       "  0.040282972,\n",
       "  0.03603576,\n",
       "  0.03556691,\n",
       "  0.035937067,\n",
       "  0.036905605,\n",
       "  0.03775813,\n",
       "  0.040432062,\n",
       "  0.041099466,\n",
       "  0.03904209,\n",
       "  0.038361594,\n",
       "  0.03775898,\n",
       "  0.03715334,\n",
       "  0.040603105,\n",
       "  0.038051613,\n",
       "  0.03821904,\n",
       "  0.03956707,\n",
       "  0.038766872,\n",
       "  0.039905526,\n",
       "  0.039235525,\n",
       "  0.037698008,\n",
       "  0.036577243,\n",
       "  0.038130097,\n",
       "  0.036570355,\n",
       "  0.037962765,\n",
       "  0.041103248,\n",
       "  0.03762096,\n",
       "  0.036979165,\n",
       "  0.034907315,\n",
       "  0.03896232,\n",
       "  0.040715836,\n",
       "  0.040860366,\n",
       "  0.03745548,\n",
       "  0.0402492,\n",
       "  0.04602055,\n",
       "  0.040069934,\n",
       "  0.0394643,\n",
       "  0.036636017,\n",
       "  0.039401077,\n",
       "  0.039238743,\n",
       "  0.040987343,\n",
       "  0.041546427,\n",
       "  0.037375893,\n",
       "  0.03972267,\n",
       "  0.037641,\n",
       "  0.037024908,\n",
       "  0.036823872,\n",
       "  0.036582917,\n",
       "  0.03904764,\n",
       "  0.034440584,\n",
       "  0.036764275,\n",
       "  0.03737911,\n",
       "  0.039921477,\n",
       "  0.037596226,\n",
       "  0.035062417,\n",
       "  0.04296328,\n",
       "  0.03890213,\n",
       "  0.036917347,\n",
       "  0.03774251,\n",
       "  0.03904515,\n",
       "  0.0408472,\n",
       "  0.03855462,\n",
       "  0.038065247,\n",
       "  0.038768344,\n",
       "  0.037653822,\n",
       "  0.035122912,\n",
       "  0.035842262,\n",
       "  0.03735765,\n",
       "  0.040786687,\n",
       "  0.037120946,\n",
       "  0.03932535,\n",
       "  0.036899716,\n",
       "  0.036171105,\n",
       "  0.03673056,\n",
       "  0.03813252,\n",
       "  0.04305826,\n",
       "  0.044877503,\n",
       "  0.037276164,\n",
       "  0.040005352,\n",
       "  0.03780708,\n",
       "  0.039222483,\n",
       "  0.03440612,\n",
       "  0.035605934,\n",
       "  0.03499071,\n",
       "  0.039835725,\n",
       "  0.03665753,\n",
       "  0.037472043,\n",
       "  0.039591517,\n",
       "  0.040741254,\n",
       "  0.038460422,\n",
       "  0.035198916,\n",
       "  0.03625019,\n",
       "  0.03501744,\n",
       "  0.0370961,\n",
       "  0.039082512,\n",
       "  0.034358855,\n",
       "  0.036792308,\n",
       "  0.037968088,\n",
       "  0.034626357,\n",
       "  0.042875715,\n",
       "  0.037583046,\n",
       "  0.041138764,\n",
       "  0.033266865,\n",
       "  0.03936773,\n",
       "  0.035079002,\n",
       "  0.035508882,\n",
       "  0.03544115,\n",
       "  0.036484804,\n",
       "  0.03784457,\n",
       "  0.036733758,\n",
       "  0.038442355,\n",
       "  0.036628738,\n",
       "  0.0371418,\n",
       "  0.035309978,\n",
       "  0.037526447,\n",
       "  0.03917069,\n",
       "  0.03577731,\n",
       "  0.03468014,\n",
       "  0.03696273,\n",
       "  0.03526209,\n",
       "  0.037522245,\n",
       "  0.03752346,\n",
       "  0.03820559,\n",
       "  0.034975365,\n",
       "  0.034990914,\n",
       "  0.037884682,\n",
       "  0.034981016,\n",
       "  0.035923574,\n",
       "  0.035746027,\n",
       "  0.04119183,\n",
       "  0.046335388,\n",
       "  0.03976901,\n",
       "  0.035864577,\n",
       "  0.038928572,\n",
       "  0.039286103,\n",
       "  0.042736035,\n",
       "  0.04018488,\n",
       "  0.03666617,\n",
       "  0.03669185,\n",
       "  0.039844055,\n",
       "  0.04134455,\n",
       "  0.037784666,\n",
       "  0.0368133,\n",
       "  0.034627758,\n",
       "  0.034230262,\n",
       "  0.033940583,\n",
       "  0.036165055,\n",
       "  0.035201933,\n",
       "  0.035792053,\n",
       "  0.036845464,\n",
       "  0.037099887,\n",
       "  0.03594729,\n",
       "  0.036735415,\n",
       "  0.043476168,\n",
       "  0.039298773,\n",
       "  0.03974549,\n",
       "  0.03748313,\n",
       "  0.041994736,\n",
       "  0.039606344,\n",
       "  0.03732766,\n",
       "  0.03745198,\n",
       "  0.044418126,\n",
       "  0.036490604,\n",
       "  0.04001802,\n",
       "  0.03508396,\n",
       "  0.03589768,\n",
       "  0.0352708,\n",
       "  0.03520571,\n",
       "  0.03564172]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "np.savetxt(r'F:/12046.txt',history_df, delimiter=',')\n",
    "\n",
    "\n",
    "species_prediction=model.predict(X_testing)\n",
    "np.savetxt(r'F:/12047.txt',Y_testing, delimiter=',')\n",
    "np.savetxt(r'F:/12048.txt',species_prediction, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>1659.88574</th>\n",
       "      <th>1658.89844</th>\n",
       "      <th>1657.91016</th>\n",
       "      <th>1656.92285</th>\n",
       "      <th>1655.93457</th>\n",
       "      <th>1654.94629</th>\n",
       "      <th>1653.95703</th>\n",
       "      <th>1652.96875</th>\n",
       "      <th>1651.97949</th>\n",
       "      <th>...</th>\n",
       "      <th>549.84863</th>\n",
       "      <th>548.61035</th>\n",
       "      <th>547.37305</th>\n",
       "      <th>546.13477</th>\n",
       "      <th>544.89648</th>\n",
       "      <th>543.65723</th>\n",
       "      <th>542.41895</th>\n",
       "      <th>541.17969</th>\n",
       "      <th>539.93945</th>\n",
       "      <th>538.7002</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004729</td>\n",
       "      <td>0.026394</td>\n",
       "      <td>0.054909</td>\n",
       "      <td>0.080820</td>\n",
       "      <td>0.104126</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.142922</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.171306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123257</td>\n",
       "      <td>0.125757</td>\n",
       "      <td>0.129259</td>\n",
       "      <td>0.133763</td>\n",
       "      <td>0.139267</td>\n",
       "      <td>0.145774</td>\n",
       "      <td>0.153283</td>\n",
       "      <td>0.161795</td>\n",
       "      <td>0.171308</td>\n",
       "      <td>0.181823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035686</td>\n",
       "      <td>0.057156</td>\n",
       "      <td>0.076754</td>\n",
       "      <td>0.094484</td>\n",
       "      <td>0.110344</td>\n",
       "      <td>0.124332</td>\n",
       "      <td>0.136452</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.155081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152689</td>\n",
       "      <td>0.155453</td>\n",
       "      <td>0.159072</td>\n",
       "      <td>0.163545</td>\n",
       "      <td>0.168871</td>\n",
       "      <td>0.175054</td>\n",
       "      <td>0.182090</td>\n",
       "      <td>0.189982</td>\n",
       "      <td>0.198728</td>\n",
       "      <td>0.208331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010838</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.038850</td>\n",
       "      <td>0.050619</td>\n",
       "      <td>0.060895</td>\n",
       "      <td>0.069678</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>0.082770</td>\n",
       "      <td>0.087077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071255</td>\n",
       "      <td>0.074881</td>\n",
       "      <td>0.078365</td>\n",
       "      <td>0.081705</td>\n",
       "      <td>0.084901</td>\n",
       "      <td>0.087954</td>\n",
       "      <td>0.090864</td>\n",
       "      <td>0.093631</td>\n",
       "      <td>0.096254</td>\n",
       "      <td>0.098735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>0.008617</td>\n",
       "      <td>0.016806</td>\n",
       "      <td>0.024219</td>\n",
       "      <td>0.030853</td>\n",
       "      <td>0.036710</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>0.046091</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046010</td>\n",
       "      <td>0.046660</td>\n",
       "      <td>0.047307</td>\n",
       "      <td>0.047953</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.049237</td>\n",
       "      <td>0.049876</td>\n",
       "      <td>0.050513</td>\n",
       "      <td>0.051148</td>\n",
       "      <td>0.051782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034040</td>\n",
       "      <td>0.036216</td>\n",
       "      <td>0.037904</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>0.039787</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>0.037806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099963</td>\n",
       "      <td>0.095002</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.083937</td>\n",
       "      <td>0.077833</td>\n",
       "      <td>0.071348</td>\n",
       "      <td>0.064483</td>\n",
       "      <td>0.057238</td>\n",
       "      <td>0.049611</td>\n",
       "      <td>0.041604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1631 rows × 1016 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      con  1659.88574  1658.89844  1657.91016  1656.92285  1655.93457  \\\n",
       "0     0.0   -0.004729    0.026394    0.054909    0.080820    0.104126   \n",
       "1     0.0    0.035686    0.057156    0.076754    0.094484    0.110344   \n",
       "2     0.0    0.010838    0.025590    0.038850    0.050619    0.060895   \n",
       "3     0.0   -0.000351    0.008617    0.016806    0.024219    0.030853   \n",
       "4     0.0    0.034040    0.036216    0.037904    0.039106    0.039820   \n",
       "...   ...         ...         ...         ...         ...         ...   \n",
       "1626  NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1627  NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1628  NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1629  NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1630  NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "      1654.94629  1653.95703  1652.96875  1651.97949  ...  549.84863  \\\n",
       "0       0.124826    0.142922    0.158416    0.171306  ...   0.123257   \n",
       "1       0.124332    0.136452    0.146702    0.155081  ...   0.152689   \n",
       "2       0.069678    0.076970    0.082770    0.087077  ...   0.071255   \n",
       "3       0.036710    0.041789    0.046091    0.049615  ...   0.046010   \n",
       "4       0.040046    0.039787    0.039040    0.037806  ...   0.099963   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "1626         NaN         NaN         NaN         NaN  ...        NaN   \n",
       "1627         NaN         NaN         NaN         NaN  ...        NaN   \n",
       "1628         NaN         NaN         NaN         NaN  ...        NaN   \n",
       "1629         NaN         NaN         NaN         NaN  ...        NaN   \n",
       "1630         NaN         NaN         NaN         NaN  ...        NaN   \n",
       "\n",
       "      548.61035  547.37305  546.13477  544.89648  543.65723  542.41895  \\\n",
       "0      0.125757   0.129259   0.133763   0.139267   0.145774   0.153283   \n",
       "1      0.155453   0.159072   0.163545   0.168871   0.175054   0.182090   \n",
       "2      0.074881   0.078365   0.081705   0.084901   0.087954   0.090864   \n",
       "3      0.046660   0.047307   0.047953   0.048596   0.049237   0.049876   \n",
       "4      0.095002   0.089661   0.083937   0.077833   0.071348   0.064483   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1626        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1627        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1628        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1629        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1630        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "      541.17969  539.93945  538.7002  \n",
       "0      0.161795   0.171308  0.181823  \n",
       "1      0.189982   0.198728  0.208331  \n",
       "2      0.093631   0.096254  0.098735  \n",
       "3      0.050513   0.051148  0.051782  \n",
       "4      0.057238   0.049611  0.041604  \n",
       "...         ...        ...       ...  \n",
       "1626        NaN        NaN       NaN  \n",
       "1627        NaN        NaN       NaN  \n",
       "1628        NaN        NaN       NaN  \n",
       "1629        NaN        NaN       NaN  \n",
       "1630        NaN        NaN       NaN  \n",
       "\n",
       "[1631 rows x 1016 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预测\n",
    "\n",
    "test=pd.read_csv('./cellRNA.csv', engine='python')\n",
    "\n",
    "X, Y=prepare_x(test)\n",
    "#经此之后，输入文件变成标准的标签-特征模式\n",
    "\n",
    "nb_features = 1015 \n",
    "X_test = np.zeros((len(X), nb_features, 1))\n",
    "X_test[:, :, 0] = X[:, :nb_features]\n",
    "#经此之后，改变输入特征格式\n",
    "\n",
    "species_prediction=model.predict(X_test)\n",
    "np.savetxt(r'F:/0220.txt',species_prediction, delimiter=',')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-91fbfa5cfabc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mobtain_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtest_predict_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobtain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_testing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 492\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    756\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 574\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "#线性回归做对照\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "model = LinearRegression()  \n",
    "\n",
    "obtain_model=model.fit(X_t,Y_t)\n",
    "\n",
    "test_predict_label = obtain_model.predict(X_testing)\n",
    "\n",
    "test_predict_label = np.array(test_predict_label)\n",
    "\n",
    "np.savetxt(r'F:/12131.txt',test_predict_label, delimiter=',')\n",
    "np.savetxt(r'F:/12132.txt',Y_testing, delimiter=',')\n",
    "\n",
    "mae = mean_absolute_error(Y_testing, test_predict_label)\n",
    "mse = mean_squared_error(Y_testing, test_predict_label)\n",
    "\n",
    "print(f'线性回归的MAE: {mae}')\n",
    "print(f'线性回归的MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pls-da回归做对照\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "plsda = PLSRegression()\n",
    "\n",
    "n_components_values = list(np.arange(1, 101))\n",
    "\n",
    "param_grid = {'n_components': n_components_values}\n",
    "\n",
    "grid_search = GridSearchCV(plsda, param_grid, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "# 执行网格搜索\n",
    "grid_search.fit(X_t, Y_t)\n",
    "\n",
    "# 获取最佳主成分数量\n",
    "best_n_components = grid_search.best_params_['n_components']\n",
    "\n",
    "print(f'最佳主成分数量: {best_n_components}')\n",
    "\n",
    "# 使用最佳参数重新训练模型\n",
    "best_plsda = PLSRegression(n_components=best_n_components)\n",
    "best_plsda.fit(X_t, Y_t)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred = best_plsda.predict(X_testing)\n",
    "\n",
    "np.savetxt(r'F:/12135.txt',y_pred, delimiter=',')\n",
    "np.savetxt(r'F:/12136.txt',Y_testing, delimiter=',')\n",
    "\n",
    "mae = mean_absolute_error(Y_testing, y_pred)\n",
    "mse = mean_squared_error(Y_testing, y_pred)\n",
    "\n",
    "print(f'线性回归的MAE: {mae}')\n",
    "print(f'线性回归的MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
